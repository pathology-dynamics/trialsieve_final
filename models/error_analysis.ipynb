{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>pmid</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>tag</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>correct</th>\n",
       "      <th>error_type</th>\n",
       "      <th>annotator</th>\n",
       "      <th>correction</th>\n",
       "      <th>remarks</th>\n",
       "      <th>confidence level (1-5)</th>\n",
       "      <th>**1 is not confident at all, 5 is absolutely confident**</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>58651</td>\n",
       "      <td>26</td>\n",
       "      <td>42</td>\n",
       "      <td>Group Characteristic</td>\n",
       "      <td>renal transplant</td>\n",
       "      <td>model</td>\n",
       "      <td>y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ccurtis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>58651</td>\n",
       "      <td>144</td>\n",
       "      <td>149</td>\n",
       "      <td>Drug Intervention</td>\n",
       "      <td>HAHTG</td>\n",
       "      <td>human</td>\n",
       "      <td>y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ccurtis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>58651</td>\n",
       "      <td>165</td>\n",
       "      <td>175</td>\n",
       "      <td>Non-Study Drug</td>\n",
       "      <td>prednisone</td>\n",
       "      <td>human</td>\n",
       "      <td>y</td>\n",
       "      <td>wrong label</td>\n",
       "      <td>ccurtis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>58651</td>\n",
       "      <td>180</td>\n",
       "      <td>192</td>\n",
       "      <td>Non-Study Drug</td>\n",
       "      <td>azathioprine</td>\n",
       "      <td>human</td>\n",
       "      <td>y</td>\n",
       "      <td>wrong label</td>\n",
       "      <td>ccurtis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>58651</td>\n",
       "      <td>194</td>\n",
       "      <td>200</td>\n",
       "      <td>Drug Intervention</td>\n",
       "      <td>lmuran</td>\n",
       "      <td>model</td>\n",
       "      <td>n</td>\n",
       "      <td>wrong label</td>\n",
       "      <td>ccurtis</td>\n",
       "      <td>Non-Study Drug</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5561</th>\n",
       "      <td>5561</td>\n",
       "      <td>36206137</td>\n",
       "      <td>2226</td>\n",
       "      <td>2228</td>\n",
       "      <td>Quantitative Measurement</td>\n",
       "      <td>53</td>\n",
       "      <td>human</td>\n",
       "      <td>n</td>\n",
       "      <td>too little</td>\n",
       "      <td>eduvaris</td>\n",
       "      <td>0.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5562</th>\n",
       "      <td>5562</td>\n",
       "      <td>36206137</td>\n",
       "      <td>2226</td>\n",
       "      <td>2230</td>\n",
       "      <td>Quantitative Measurement</td>\n",
       "      <td>53â€Š%</td>\n",
       "      <td>model</td>\n",
       "      <td>y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eduvaris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>5563</td>\n",
       "      <td>36206137</td>\n",
       "      <td>2235</td>\n",
       "      <td>2237</td>\n",
       "      <td>Quantitative Measurement</td>\n",
       "      <td>47</td>\n",
       "      <td>human</td>\n",
       "      <td>n</td>\n",
       "      <td>too little</td>\n",
       "      <td>eduvaris</td>\n",
       "      <td>0.47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>5564</td>\n",
       "      <td>36206137</td>\n",
       "      <td>2235</td>\n",
       "      <td>2239</td>\n",
       "      <td>Quantitative Measurement</td>\n",
       "      <td>47â€Š%</td>\n",
       "      <td>model</td>\n",
       "      <td>y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eduvaris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>5565</td>\n",
       "      <td>36206137</td>\n",
       "      <td>2288</td>\n",
       "      <td>2292</td>\n",
       "      <td>Quantitative Measurement</td>\n",
       "      <td>94â€Š%</td>\n",
       "      <td>model</td>\n",
       "      <td>y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eduvaris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5566 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0      pmid  start   end                       tag  \\\n",
       "0              0     58651     26    42      Group Characteristic   \n",
       "1              1     58651    144   149         Drug Intervention   \n",
       "2              2     58651    165   175            Non-Study Drug   \n",
       "3              3     58651    180   192            Non-Study Drug   \n",
       "4              4     58651    194   200         Drug Intervention   \n",
       "...          ...       ...    ...   ...                       ...   \n",
       "5561        5561  36206137   2226  2228  Quantitative Measurement   \n",
       "5562        5562  36206137   2226  2230  Quantitative Measurement   \n",
       "5563        5563  36206137   2235  2237  Quantitative Measurement   \n",
       "5564        5564  36206137   2235  2239  Quantitative Measurement   \n",
       "5565        5565  36206137   2288  2292  Quantitative Measurement   \n",
       "\n",
       "                  text source correct   error_type annotator      correction  \\\n",
       "0     renal transplant  model       y          NaN   ccurtis             NaN   \n",
       "1                HAHTG  human       y          NaN   ccurtis             NaN   \n",
       "2           prednisone  human       y  wrong label   ccurtis             NaN   \n",
       "3         azathioprine  human       y  wrong label   ccurtis             NaN   \n",
       "4               lmuran  model       n  wrong label   ccurtis  Non-Study Drug   \n",
       "...                ...    ...     ...          ...       ...             ...   \n",
       "5561                53  human       n   too little  eduvaris            0.53   \n",
       "5562            53â€Š%  model       y          NaN  eduvaris             NaN   \n",
       "5563                47  human       n   too little  eduvaris            0.47   \n",
       "5564            47â€Š%  model       y          NaN  eduvaris             NaN   \n",
       "5565            94â€Š%  model       y          NaN  eduvaris             NaN   \n",
       "\n",
       "     remarks  confidence level (1-5)  \\\n",
       "0        NaN                     NaN   \n",
       "1        NaN                     NaN   \n",
       "2        NaN                     NaN   \n",
       "3        NaN                     NaN   \n",
       "4        NaN                     4.0   \n",
       "...      ...                     ...   \n",
       "5561     NaN                     5.0   \n",
       "5562     NaN                     NaN   \n",
       "5563     NaN                     5.0   \n",
       "5564     NaN                     NaN   \n",
       "5565     NaN                     NaN   \n",
       "\n",
       "     **1 is not confident at all, 5 is absolutely confident**  \n",
       "0                                                   NaN        \n",
       "1                                                   NaN        \n",
       "2                                                   NaN        \n",
       "3                                                   NaN        \n",
       "4                                                   NaN        \n",
       "...                                                 ...        \n",
       "5561                                                NaN        \n",
       "5562                                                NaN        \n",
       "5563                                                NaN        \n",
       "5564                                                NaN        \n",
       "5565                                                NaN        \n",
       "\n",
       "[5566 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/cleaned_error_analysis.csv')\n",
    "df = pd.read_excel('../data/cleaned_error_analysis_v2.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Number of tagged mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model tagged mentions : 3103\n",
      "Number of human tagged mentions : 2463\n"
     ]
    }
   ],
   "source": [
    "df_model = df[df['source'] == 'model']\n",
    "df_human = df[df['source'] == 'human']\n",
    "print(\"Number of model tagged mentions :\", len(df_model))\n",
    "print(\"Number of human tagged mentions :\", len(df_human))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of pmids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model unique PMIDs: 237\n",
      "Human unique PMIDs: 236\n"
     ]
    }
   ],
   "source": [
    "model_unique_pmids = df_model['pmid'].nunique()\n",
    "human_unique_pmids = df_human['pmid'].nunique()\n",
    "print(f\"Model unique PMIDs: {model_unique_pmids}\")\n",
    "print(f\"Human unique PMIDs: {human_unique_pmids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different pmids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pmids that are not in common are : {8151609, 2719459, 3582605}\n",
      "Model was evaluated on {8151609, 3582605}  that human was not evaluated on\n",
      "Human was evaluated on {2719459}  that model was not evaluated on\n"
     ]
    }
   ],
   "source": [
    "pmid_difference1 = set(df_model['pmid']) - set(df_human['pmid'])\n",
    "pmid_difference2 = set(df_human['pmid']) - set(df_model['pmid'])\n",
    "pmid_difference = pmid_difference1.union(pmid_difference2)\n",
    "print(\"The pmids that are not in common are :\", pmid_difference)\n",
    "print(\"Model was evaluated on\", pmid_difference1, \" that human was not evaluated on\")\n",
    "print(\"Human was evaluated on\", pmid_difference2, \" that model was not evaluated on\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different error types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multiple entities',\n",
       " nan,\n",
       " 'not capturable',\n",
       " 'not needed',\n",
       " 'too little',\n",
       " 'too little or too much based on context',\n",
       " 'too much',\n",
       " 'wrong label'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_model['error_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of correctly labeled mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly labeled mentions by the model: 2218, 71.47921366419594%\n",
      "Number of correctly labeled mentions by the human: 1240, 50.345107592367036%\n"
     ]
    }
   ],
   "source": [
    "model_correct_label_count = df_model['error_type'].isna().sum() - (df_model[(df_model['correct'] == 'n') & (df_model['error_type'].isna())].shape[0])\n",
    "human_correct_label_count = df_human['error_type'].isna().sum() - (df_human[(df_human['correct'] == 'n') & (df_human['error_type'].isna())].shape[0])\n",
    "print(f\"Number of correctly labeled mentions by the model: {model_correct_label_count}, {model_correct_label_count/len(df_model)*100}%\")\n",
    "print(f\"Number of correctly labeled mentions by the human: {human_correct_label_count}, {human_correct_label_count/len(df_human)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of uncorrectly labeled mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly tagged mentions but uncorrectly labeled by the model: 885, 28.52078633580406%\n",
      "Number of correctly tagged mentions but uncorrectly labeled by the human: 1223, 49.654892407632964%\n"
     ]
    }
   ],
   "source": [
    "model_uncorrect_label_count = df_model['error_type'].notna().sum() + (df_model[(df_model['correct'] == 'n') & (df_model['error_type'].isna())].shape[0])\n",
    "human_uncorrect_label_count = df_human['error_type'].notna().sum() + (df_human[(df_human['correct'] == 'n') & (df_human['error_type'].isna())].shape[0])\n",
    "print(f\"Number of correctly tagged mentions but uncorrectly labeled by the model: {model_uncorrect_label_count}, {model_uncorrect_label_count/len(df_model)*100}%\")\n",
    "print(f\"Number of correctly tagged mentions but uncorrectly labeled by the human: {human_uncorrect_label_count}, {human_uncorrect_label_count/len(df_human)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type of error : wrong label / too much / too little / multiple entities / not needed (= shouldn't have been tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mentions that got 'wrong label' from the model: 302, 34.12429378531073%\n",
      "Number of mentions that got 'wrong label' from the human: 481, 39.329517579721994%\n",
      "Number of mentions that got 'too little' from the model: 239, 27.005649717514125%\n",
      "Number of mentions that got 'too little' from the human: 279, 22.812755519215045%\n",
      "Number of mentions that got 'not needed' by the model: 141, 15.932203389830507%\n",
      "Number of mentions that got 'not needed' by the human: 129, 10.54783319705642%\n",
      "Number of mentions that got 'too much' from the model: 129, 14.576271186440678%\n",
      "Number of mentions that got 'too much' from the human: 219, 17.906786590351594%\n",
      "Number of mentions that got 'multiple entities' from the model: 41, 4.632768361581921%\n",
      "Number of mentions that got 'multiple entities' from the human: 79, 6.459525756336877%\n",
      "Number of mentions that got 'too little or too much based on context' from the model: 14, 1.5819209039548021%\n",
      "Number of mentions that got 'too little or too much based on context' from the human: 8, 0.6541291905151267%\n",
      "Number of mentions that shouldn't have been tagged by the model: 18, 2.0338983050847457%\n",
      "Number of mentions that shouldn't have been tagged by the human: 23, 1.8806214227309894%\n"
     ]
    }
   ],
   "source": [
    "model_wrong_label_count = df_model[df_model['error_type'] == 'wrong label'].shape[0]\n",
    "human_wrong_label_count = df_human[df_human['error_type'] == 'wrong label'].shape[0]\n",
    "print(f\"Number of mentions that got 'wrong label' from the model: {model_wrong_label_count}, {model_wrong_label_count/model_uncorrect_label_count*100}%\")\n",
    "print(f\"Number of mentions that got 'wrong label' from the human: {human_wrong_label_count}, {human_wrong_label_count/human_uncorrect_label_count*100}%\")\n",
    "\n",
    "model_too_little_count = df_model[df_model['error_type'] == 'too little'].shape[0]\n",
    "human_too_little_count = df_human[df_human['error_type'] == 'too little'].shape[0]\n",
    "print(f\"Number of mentions that got 'too little' from the model: {model_too_little_count}, {model_too_little_count/model_uncorrect_label_count*100}%\")\n",
    "print(f\"Number of mentions that got 'too little' from the human: {human_too_little_count}, {human_too_little_count/human_uncorrect_label_count*100}%\")\n",
    "\n",
    "model_not_needed_tag_count = df_model[(df_model['error_type'] == 'not needed')].shape[0]\n",
    "human_not_needed_tag_count = df_human[(df_human['error_type'] == 'not needed')].shape[0]\n",
    "print(f\"Number of mentions that got 'not needed' by the model: {model_not_needed_tag_count}, {model_not_needed_tag_count/model_uncorrect_label_count*100}%\")\n",
    "print(f\"Number of mentions that got 'not needed' by the human: {human_not_needed_tag_count}, {human_not_needed_tag_count/human_uncorrect_label_count*100}%\")\n",
    "\n",
    "model_too_much_count = df_model[df_model['error_type'] == 'too much'].shape[0]\n",
    "human_too_much_count = df_human[df_human['error_type'] == 'too much'].shape[0]\n",
    "print(f\"Number of mentions that got 'too much' from the model: {model_too_much_count}, {model_too_much_count/model_uncorrect_label_count*100}%\")\n",
    "print(f\"Number of mentions that got 'too much' from the human: {human_too_much_count}, {human_too_much_count/human_uncorrect_label_count*100}%\")\n",
    "\n",
    "model_multiple_entities_count = df_model[df_model['error_type'] == 'multiple entities'].shape[0]\n",
    "human_multiple_entities_count = df_human[df_human['error_type'] == 'multiple entities'].shape[0]\n",
    "print(f\"Number of mentions that got 'multiple entities' from the model: {model_multiple_entities_count}, {model_multiple_entities_count/model_uncorrect_label_count*100}%\")\n",
    "print(f\"Number of mentions that got 'multiple entities' from the human: {human_multiple_entities_count}, {human_multiple_entities_count/human_uncorrect_label_count*100}%\")\n",
    "\n",
    "model_little_much_count = df_model[df_model['error_type'] == 'too little or too much based on context'].shape[0]\n",
    "human_little_much_count = df_human[df_human['error_type'] == 'too little or too much based on context'].shape[0]\n",
    "print(f\"Number of mentions that got 'too little or too much based on context' from the model: {model_little_much_count}, {model_little_much_count/model_uncorrect_label_count*100}%\")\n",
    "print(f\"Number of mentions that got 'too little or too much based on context' from the human: {human_little_much_count}, {human_little_much_count/human_uncorrect_label_count*100}%\")\n",
    "\n",
    "model_empty_count = (df_model[(df_model['correct'] == 'n') & (df_model['error_type'].isna())].shape[0])\n",
    "human_empty_count = (df_human[(df_human['correct'] == 'n') & (df_human['error_type'].isna())].shape[0])\n",
    "print(f\"Number of mentions that shouldn't have been tagged by the model: {model_empty_count}, {model_empty_count/model_uncorrect_label_count*100}%\")\n",
    "print(f\"Number of mentions that shouldn't have been tagged by the human: {human_empty_count}, {human_empty_count/human_uncorrect_label_count*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "error_type\n",
       "wrong label                                302\n",
       "too little                                 239\n",
       "not needed                                 141\n",
       "too much                                   129\n",
       "multiple entities                           41\n",
       "too little or too much based on context     14\n",
       "not capturable                               1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_model['error_type'].value_counts().sum())\n",
    "df_model['error_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "error_type\n",
       "wrong label                                       481\n",
       "too little                                        279\n",
       "too much                                          219\n",
       "not needed                                        129\n",
       "multiple entities                                  79\n",
       "too little or too much based on context             8\n",
       "too much or multiple entities based on context      3\n",
       "not necessary/wrong label                           2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_human['error_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trialsieve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
