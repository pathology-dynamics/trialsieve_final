{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>pmid</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>tag</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>correct</th>\n",
       "      <th>error_type</th>\n",
       "      <th>annotator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>58651</td>\n",
       "      <td>26</td>\n",
       "      <td>42</td>\n",
       "      <td>Group Characteristic</td>\n",
       "      <td>renal transplant</td>\n",
       "      <td>model</td>\n",
       "      <td>y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ccurtis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>58651</td>\n",
       "      <td>144</td>\n",
       "      <td>149</td>\n",
       "      <td>Drug Intervention</td>\n",
       "      <td>HAHTG</td>\n",
       "      <td>human</td>\n",
       "      <td>y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ccurtis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>58651</td>\n",
       "      <td>165</td>\n",
       "      <td>175</td>\n",
       "      <td>Non-Study Drug</td>\n",
       "      <td>prednisone</td>\n",
       "      <td>human</td>\n",
       "      <td>y</td>\n",
       "      <td>wrong label</td>\n",
       "      <td>ccurtis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>58651</td>\n",
       "      <td>180</td>\n",
       "      <td>192</td>\n",
       "      <td>Non-Study Drug</td>\n",
       "      <td>azathioprine</td>\n",
       "      <td>human</td>\n",
       "      <td>y</td>\n",
       "      <td>wrong label</td>\n",
       "      <td>ccurtis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>58651</td>\n",
       "      <td>194</td>\n",
       "      <td>200</td>\n",
       "      <td>Drug Intervention</td>\n",
       "      <td>lmuran</td>\n",
       "      <td>model</td>\n",
       "      <td>n</td>\n",
       "      <td>wrong label</td>\n",
       "      <td>ccurtis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5561</th>\n",
       "      <td>5561</td>\n",
       "      <td>36206137</td>\n",
       "      <td>2226</td>\n",
       "      <td>2228</td>\n",
       "      <td>Quantitative Measurement</td>\n",
       "      <td>53</td>\n",
       "      <td>human</td>\n",
       "      <td>n</td>\n",
       "      <td>too little</td>\n",
       "      <td>eduvaris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5562</th>\n",
       "      <td>5562</td>\n",
       "      <td>36206137</td>\n",
       "      <td>2226</td>\n",
       "      <td>2230</td>\n",
       "      <td>Quantitative Measurement</td>\n",
       "      <td>53 %</td>\n",
       "      <td>model</td>\n",
       "      <td>y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eduvaris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>5563</td>\n",
       "      <td>36206137</td>\n",
       "      <td>2235</td>\n",
       "      <td>2237</td>\n",
       "      <td>Quantitative Measurement</td>\n",
       "      <td>47</td>\n",
       "      <td>human</td>\n",
       "      <td>n</td>\n",
       "      <td>too little</td>\n",
       "      <td>eduvaris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>5564</td>\n",
       "      <td>36206137</td>\n",
       "      <td>2235</td>\n",
       "      <td>2239</td>\n",
       "      <td>Quantitative Measurement</td>\n",
       "      <td>47 %</td>\n",
       "      <td>model</td>\n",
       "      <td>y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eduvaris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>5565</td>\n",
       "      <td>36206137</td>\n",
       "      <td>2288</td>\n",
       "      <td>2292</td>\n",
       "      <td>Quantitative Measurement</td>\n",
       "      <td>94 %</td>\n",
       "      <td>model</td>\n",
       "      <td>y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eduvaris</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5566 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0      pmid  start   end                       tag  \\\n",
       "0              0     58651     26    42      Group Characteristic   \n",
       "1              1     58651    144   149         Drug Intervention   \n",
       "2              2     58651    165   175            Non-Study Drug   \n",
       "3              3     58651    180   192            Non-Study Drug   \n",
       "4              4     58651    194   200         Drug Intervention   \n",
       "...          ...       ...    ...   ...                       ...   \n",
       "5561        5561  36206137   2226  2228  Quantitative Measurement   \n",
       "5562        5562  36206137   2226  2230  Quantitative Measurement   \n",
       "5563        5563  36206137   2235  2237  Quantitative Measurement   \n",
       "5564        5564  36206137   2235  2239  Quantitative Measurement   \n",
       "5565        5565  36206137   2288  2292  Quantitative Measurement   \n",
       "\n",
       "                  text source correct   error_type annotator  \n",
       "0     renal transplant  model       y          NaN   ccurtis  \n",
       "1                HAHTG  human       y          NaN   ccurtis  \n",
       "2           prednisone  human       y  wrong label   ccurtis  \n",
       "3         azathioprine  human       y  wrong label   ccurtis  \n",
       "4               lmuran  model       n  wrong label   ccurtis  \n",
       "...                ...    ...     ...          ...       ...  \n",
       "5561                53  human       n   too little  eduvaris  \n",
       "5562              53 %  model       y          NaN  eduvaris  \n",
       "5563                47  human       n   too little  eduvaris  \n",
       "5564              47 %  model       y          NaN  eduvaris  \n",
       "5565              94 %  model       y          NaN  eduvaris  \n",
       "\n",
       "[5566 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_error_analysis.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Number of tagged mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model tagged mentions : 3103\n",
      "Number of human tagged mentions : 2463\n"
     ]
    }
   ],
   "source": [
    "df_model = df[df['source'] == 'model']\n",
    "df_human = df[df['source'] == 'human']\n",
    "print(\"Number of model tagged mentions :\", len(df_model))\n",
    "print(\"Number of human tagged mentions :\", len(df_human))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of pmids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model unique PMIDs: 237\n",
      "Human unique PMIDs: 236\n"
     ]
    }
   ],
   "source": [
    "model_unique_pmids = df_model['pmid'].nunique()\n",
    "human_unique_pmids = df_human['pmid'].nunique()\n",
    "print(f\"Model unique PMIDs: {model_unique_pmids}\")\n",
    "print(f\"Human unique PMIDs: {human_unique_pmids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different pmids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pmids that are not in common are : {8151609, 2719459, 3582605}\n",
      "Model was evaluated on {8151609, 3582605}  that human was not evaluated on\n",
      "Human was evaluated on {2719459}  that model was not evaluated on\n"
     ]
    }
   ],
   "source": [
    "pmid_difference1 = set(df_model['pmid']) - set(df_human['pmid'])\n",
    "pmid_difference2 = set(df_human['pmid']) - set(df_model['pmid'])\n",
    "pmid_difference = pmid_difference1.union(pmid_difference2)\n",
    "print(\"The pmids that are not in common are :\", pmid_difference)\n",
    "print(\"Model was evaluated on\", pmid_difference1, \" that human was not evaluated on\")\n",
    "print(\"Human was evaluated on\", pmid_difference2, \" that model was not evaluated on\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different error types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multiple entities',\n",
       " nan,\n",
       " 'not capturable',\n",
       " 'not needed',\n",
       " 'too little',\n",
       " 'too little or too much based on context',\n",
       " 'too much',\n",
       " 'wrong label'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_model['error_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of correctly labeled mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly labeled mentions by the model: 2218, 71.47921366419594%\n",
      "Number of correctly labeled mentions by the human: 1240, 50.345107592367036%\n"
     ]
    }
   ],
   "source": [
    "model_correct_label_count = df_model['error_type'].isna().sum() - (df_model[(df_model['correct'] == 'n') & (df_model['error_type'].isna())].shape[0])\n",
    "human_correct_label_count = df_human['error_type'].isna().sum() - (df_human[(df_human['correct'] == 'n') & (df_human['error_type'].isna())].shape[0])\n",
    "print(f\"Number of correctly labeled mentions by the model: {model_correct_label_count}, {model_correct_label_count/len(df_model)*100}%\")\n",
    "print(f\"Number of correctly labeled mentions by the human: {human_correct_label_count}, {human_correct_label_count/len(df_human)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of uncorrectly labeled mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly tagged mentions but uncorrectly labeled by the model: 885, 28.52078633580406%\n",
      "Number of correctly tagged mentions but uncorrectly labeled by the human: 1223, 49.654892407632964%\n"
     ]
    }
   ],
   "source": [
    "model_uncorrect_label_count = df_model['error_type'].notna().sum() + (df_model[(df_model['correct'] == 'n') & (df_model['error_type'].isna())].shape[0])\n",
    "human_uncorrect_label_count = df_human['error_type'].notna().sum() + (df_human[(df_human['correct'] == 'n') & (df_human['error_type'].isna())].shape[0])\n",
    "print(f\"Number of correctly tagged mentions but uncorrectly labeled by the model: {model_uncorrect_label_count}, {model_uncorrect_label_count/len(df_model)*100}%\")\n",
    "print(f\"Number of correctly tagged mentions but uncorrectly labeled by the human: {human_uncorrect_label_count}, {human_uncorrect_label_count/len(df_human)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type of error : wrong label / too much / too little / multiple entities / not needed (= shouldn't have been tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mentions that got 'wrong label' from the model: 302, 34.12429378531073%\n",
      "Number of mentions that got 'wrong label' from the human: 481, 39.329517579721994%\n",
      "Number of mentions that got 'too little' from the model: 239, 27.005649717514125%\n",
      "Number of mentions that got 'too little' from the human: 279, 22.812755519215045%\n",
      "Number of mentions that got 'not needed' by the model: 141, 15.932203389830507%\n",
      "Number of mentions that got 'not needed' by the human: 129, 10.54783319705642%\n",
      "Number of mentions that got 'too much' from the model: 129, 14.576271186440678%\n",
      "Number of mentions that got 'too much' from the human: 219, 17.906786590351594%\n",
      "Number of mentions that got 'multiple entities' from the model: 41, 4.632768361581921%\n",
      "Number of mentions that got 'multiple entities' from the human: 79, 6.459525756336877%\n",
      "Number of mentions that got 'too little or too much based on context' from the model: 14, 1.5819209039548021%\n",
      "Number of mentions that got 'too little or too much based on context' from the human: 8, 0.6541291905151267%\n",
      "Number of mentions that shouldn't have been tagged by the model: 18, 2.0338983050847457%\n",
      "Number of mentions that shouldn't have been tagged by the human: 23, 1.8806214227309894%\n"
     ]
    }
   ],
   "source": [
    "model_wrong_label_count = df_model[df_model['error_type'] == 'wrong label'].shape[0]\n",
    "human_wrong_label_count = df_human[df_human['error_type'] == 'wrong label'].shape[0]\n",
    "print(f\"Number of mentions that got 'wrong label' from the model: {model_wrong_label_count}, {model_wrong_label_count/model_uncorrect_label_count*100}%\")\n",
    "print(f\"Number of mentions that got 'wrong label' from the human: {human_wrong_label_count}, {human_wrong_label_count/human_uncorrect_label_count*100}%\")\n",
    "\n",
    "model_too_little_count = df_model[df_model['error_type'] == 'too little'].shape[0]\n",
    "human_too_little_count = df_human[df_human['error_type'] == 'too little'].shape[0]\n",
    "print(f\"Number of mentions that got 'too little' from the model: {model_too_little_count}, {model_too_little_count/model_uncorrect_label_count*100}%\")\n",
    "print(f\"Number of mentions that got 'too little' from the human: {human_too_little_count}, {human_too_little_count/human_uncorrect_label_count*100}%\")\n",
    "\n",
    "model_not_needed_tag_count = df_model[(df_model['error_type'] == 'not needed')].shape[0]\n",
    "human_not_needed_tag_count = df_human[(df_human['error_type'] == 'not needed')].shape[0]\n",
    "print(f\"Number of mentions that got 'not needed' by the model: {model_not_needed_tag_count}, {model_not_needed_tag_count/model_uncorrect_label_count*100}%\")\n",
    "print(f\"Number of mentions that got 'not needed' by the human: {human_not_needed_tag_count}, {human_not_needed_tag_count/human_uncorrect_label_count*100}%\")\n",
    "\n",
    "model_too_much_count = df_model[df_model['error_type'] == 'too much'].shape[0]\n",
    "human_too_much_count = df_human[df_human['error_type'] == 'too much'].shape[0]\n",
    "print(f\"Number of mentions that got 'too much' from the model: {model_too_much_count}, {model_too_much_count/model_uncorrect_label_count*100}%\")\n",
    "print(f\"Number of mentions that got 'too much' from the human: {human_too_much_count}, {human_too_much_count/human_uncorrect_label_count*100}%\")\n",
    "\n",
    "model_multiple_entities_count = df_model[df_model['error_type'] == 'multiple entities'].shape[0]\n",
    "human_multiple_entities_count = df_human[df_human['error_type'] == 'multiple entities'].shape[0]\n",
    "print(f\"Number of mentions that got 'multiple entities' from the model: {model_multiple_entities_count}, {model_multiple_entities_count/model_uncorrect_label_count*100}%\")\n",
    "print(f\"Number of mentions that got 'multiple entities' from the human: {human_multiple_entities_count}, {human_multiple_entities_count/human_uncorrect_label_count*100}%\")\n",
    "\n",
    "model_little_much_count = df_model[df_model['error_type'] == 'too little or too much based on context'].shape[0]\n",
    "human_little_much_count = df_human[df_human['error_type'] == 'too little or too much based on context'].shape[0]\n",
    "print(f\"Number of mentions that got 'too little or too much based on context' from the model: {model_little_much_count}, {model_little_much_count/model_uncorrect_label_count*100}%\")\n",
    "print(f\"Number of mentions that got 'too little or too much based on context' from the human: {human_little_much_count}, {human_little_much_count/human_uncorrect_label_count*100}%\")\n",
    "\n",
    "model_empty_count = (df_model[(df_model['correct'] == 'n') & (df_model['error_type'].isna())].shape[0])\n",
    "human_empty_count = (df_human[(df_human['correct'] == 'n') & (df_human['error_type'].isna())].shape[0])\n",
    "print(f\"Number of mentions that shouldn't have been tagged by the model: {model_empty_count}, {model_empty_count/model_uncorrect_label_count*100}%\")\n",
    "print(f\"Number of mentions that shouldn't have been tagged by the human: {human_empty_count}, {human_empty_count/human_uncorrect_label_count*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884\n",
      "885\n",
      "1218\n",
      "1223\n"
     ]
    }
   ],
   "source": [
    "print(model_not_needed_tag_count + model_empty_count + model_little_much_count + model_multiple_entities_count + model_too_little_count + model_too_much_count + model_wrong_label_count)\n",
    "print(model_uncorrect_label_count)\n",
    "print(human_little_much_count + human_multiple_entities_count + human_too_little_count + human_too_much_count + human_wrong_label_count + human_uncorrect_tag_count)\n",
    "print(human_uncorrect_label_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wrong label                                302\n",
       "too little                                 239\n",
       "not needed                                 141\n",
       "too much                                   129\n",
       "multiple entities                           41\n",
       "too little or too much based on context     14\n",
       "not capturable                               1\n",
       "Name: error_type, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_model['error_type'].value_counts().sum())\n",
    "df_model['error_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wrong label                                       481\n",
       "too little                                        279\n",
       "too much                                          219\n",
       "not needed                                        129\n",
       "multiple entities                                  79\n",
       "too little or too much based on context             8\n",
       "too much or multiple entities based on context      3\n",
       "not necessary/wrong label                           2\n",
       "Name: error_type, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_human['error_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlpacaEval : Using LLM as evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6559519826117296"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid = lambda x: 1 / (1 + (np.exp(-x)))\n",
    "f1 = lambda x1, x2 : np.cos(x1) * np.cos(x2) + sigmoid(x2)\n",
    "f2 = lambda x1, x2 : np.log(x1 + x2) + x1**2 * x2 \n",
    "\n",
    "print(f1(1,2))\n",
    "print(f2(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1_dw1=0.3512938682533773, df1_dw2=-0.38556867082343294, df2_dw1=4.352779009267449, df2_dw2=1.3327790092674707\n"
     ]
    }
   ],
   "source": [
    "# Forward\n",
    "w1, w2 = 1, 2\n",
    "dw1, dw2 = 0.01, 0.01\n",
    "df1_dw1 = (f1(w1+dw1, w2) - f1(w1, w2))/dw1\n",
    "df1_dw2 = (f1(w1, w2+dw2) - f1(w1, w2))/dw2\n",
    "\n",
    "df2_dw1 = (f2(w1+dw1, w2) - f2(w1, w2))/dw1\n",
    "df2_dw2 = (f2(w1, w2+dw2) - f2(w1, w2))/dw2\n",
    "\n",
    "print(f\"{df1_dw1=}, {df1_dw2=}, {df2_dw1=}, {df2_dw2=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1_dw1(1,2)=0.35017548837401463, df1_dw2(1,2)=Array(-0.38630188, dtype=float32), df2_dw1(1,2)=4.333333333333333, df2_dw2(1,2)=1.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "df1_dw1 = lambda x1, x2 : - np.sin(x1) * np.cos(x2)\n",
    "df1_dw2 = lambda x1, x2 : - np.cos(x1) * np.sin(x2) + sigmoid(x2) * (1 - sigmoid(x2))\n",
    "df2_dw1 = lambda x1, x2 : 1/(x1 + x2) + 2*x1*x2\n",
    "df2_dw2 = lambda x1, x2 : 1/(x1 + x2) + x1**2\n",
    "\n",
    "print(f\"{df1_dw1(1,2)=}, {df1_dw2(1,2)=}, {df2_dw1(1,2)=}, {df2_dw2(1,2)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian at w = (1, 2):\n",
      "[[ 0.35129387 -0.38556867]\n",
      " [ 4.35277901  1.33277901]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid function\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Function definitions\n",
    "def f1(w1, w2):\n",
    "    return np.cos(w1) * np.cos(w2) + sigmoid(w2)\n",
    "\n",
    "def f2(w1, w2):\n",
    "    return np.log(w1 + w2) + w1**2 * w2\n",
    "\n",
    "# Vector function f = [f1, f2]\n",
    "def f(w):\n",
    "    w1, w2 = w\n",
    "    return np.array([f1(w1, w2), f2(w1, w2)])\n",
    "\n",
    "# Numerical differentiation to compute Jacobian\n",
    "def numerical_jacobian(f, w, delta_w=0.01):\n",
    "    jacobian = np.zeros((len(f(w)), len(w)))\n",
    "    for i in range(len(w)):\n",
    "        w_plus = w.copy()\n",
    "        w_plus[i] += delta_w\n",
    "        derivative = (f(w_plus) - f(w)) / delta_w\n",
    "        jacobian[:, i] = derivative\n",
    "    return jacobian\n",
    "\n",
    "# Given w = (1, 2)\n",
    "w = np.array([1.0, 2.0])\n",
    "jacobian = numerical_jacobian(f, w)\n",
    "\n",
    "print(\"Jacobian at w = (1, 2):\")\n",
    "print(jacobian)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian forward at w = (1, 2):\n",
      "[[ 0.35017547 -0.38630188]\n",
      " [ 4.3333335   1.3333334 ]]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Define the sigmoid function\n",
    "sigmoid = lambda x: 1 / (1 + jnp.exp(-x))\n",
    "\n",
    "# Define the functions f1 and f2\n",
    "def f1(w):\n",
    "    w1, w2 = w\n",
    "    return jnp.cos(w1) * jnp.cos(w2) + sigmoid(w2)\n",
    "\n",
    "def f2(w):\n",
    "    w1, w2 = w\n",
    "    return jnp.log(w1 + w2) + w1**2 * w2\n",
    "\n",
    "# Combine f1 and f2 into a vector function f\n",
    "def f(w):\n",
    "    return jnp.array([f1(w), f2(w)])\n",
    "\n",
    "# Compute the Jacobian using JAX's jacfwd (forward-mode differentiation)\n",
    "w = jnp.array([1.0, 2.0])\n",
    "jacobian_fwd = jax.jacfwd(f)(w)\n",
    "\n",
    "print(\"Jacobian forward at w = (1, 2):\")\n",
    "print(jacobian_fwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian backward at w = (1, 2):\n",
      "[[ 0.35017547 -0.38630188]\n",
      " [ 4.3333335   1.3333334 ]]\n"
     ]
    }
   ],
   "source": [
    "# Define the sigmoid function\n",
    "sigmoid = lambda x: 1 / (1 + jnp.exp(-x))\n",
    "\n",
    "# Define the functions f1 and f2\n",
    "def f1(w):\n",
    "    w1, w2 = w\n",
    "    return jnp.cos(w1) * jnp.cos(w2) + sigmoid(w2)\n",
    "\n",
    "def f2(w):\n",
    "    w1, w2 = w\n",
    "    return jnp.log(w1 + w2) + w1**2 * w2\n",
    "\n",
    "# Combine f1 and f2 into a vector function f\n",
    "def f(w):\n",
    "    return jnp.array([f1(w), f2(w)])\n",
    "\n",
    "# Compute the Jacobian using JAX's jacrev (reverse-mode differentiation)\n",
    "w = jnp.array([1.0, 2.0])\n",
    "jacobian_rev = jax.jacrev(f)(w)\n",
    "print(\"Jacobian backward at w = (1, 2):\")\n",
    "print(jacobian_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
