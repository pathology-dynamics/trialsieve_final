{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "from ids import open_ai_api_key\n",
    "from thefuzz import fuzz, process\n",
    "import re\n",
    "import ujson\n",
    "import logging\n",
    "import inflect\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import spacy\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "openai.api_key = open_ai_api_key\n",
    "\n",
    "# seqeval evaluation\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "# spacy tokenizer\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create an engine object\n",
    "p = inflect.engine()\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method : Few-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_abstract_and_spans(path_to_processed_for_modeling, pmid):\n",
    "    ''' \n",
    "    Function to retrieve the abstract and spans from the processed_for_modeling.json file\n",
    "    ----------\n",
    "    path_to_processed_for_modeling : str\n",
    "        The path to the processed_for_modeling.json file\n",
    "    pmid : int\n",
    "        The pmid of the article to retrieve the spans\n",
    "    '''\n",
    "    with open(path_to_processed_for_modeling, \"r\") as file:\n",
    "        processed_for_modeling = ujson.load(file)\n",
    "    spans = None\n",
    "    for article in processed_for_modeling:\n",
    "        if article[\"pmid\"] == pmid:\n",
    "            abstract = article[\"text\"]\n",
    "            spans = article[\"spans\"]\n",
    "            break\n",
    "    if spans is None:\n",
    "        logging.info(\"pmid not found in the list of documents\")\n",
    "    \n",
    "    return abstract, spans\n",
    "\n",
    "    \n",
    "def tagged_abstract(path_to_processed_for_modeling, pmid):\n",
    "    '''\n",
    "    Function to retrieve the tagged abstract from the processed_for_modeling.json file\n",
    "    ----------\n",
    "    path_to_processed_for_modeling : str\n",
    "        The path to the processed_for_modeling.json file\n",
    "    pmid : int\n",
    "        The pmid of the article to retrieve the spans\n",
    "    '''\n",
    "    with open(path_to_processed_for_modeling, \"r\") as file:\n",
    "        processed_for_modeling = ujson.load(file)\n",
    "    tagged_abstract = defaultdict(list)\n",
    "    for article in processed_for_modeling:\n",
    "        if article[\"pmid\"] == pmid:\n",
    "            for span in article[\"spans\"]:\n",
    "                tagged_abstract[span[\"tag\"]].append(span[\"text\"])\n",
    "    return tagged_abstract\n",
    "\n",
    "\n",
    "def create_spans_with_surrounding_text(text, spans):\n",
    "    '''\n",
    "    This function creates a new list of spans with the surrounding text (+- 1 word) of each span\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    text : str\n",
    "        The abstract text\n",
    "    spans : list of dict\n",
    "        The spans extracted from the text : start, end, label, tag and text\n",
    "    '''\n",
    "    new_spans = []\n",
    "    # Use regex to split the text into words, including punctuation as separate tokens\n",
    "    words_with_indices = [(m.group(0), m.start(), m.end()) for m in re.finditer(r'\\S+|\\s+', text)]\n",
    "\n",
    "    def find_word_indices(span_start, span_end):\n",
    "        start_word_index = 0\n",
    "        end_word_index = 0\n",
    "        for i, (word, start_idx, end_idx) in enumerate(words_with_indices):\n",
    "            if start_idx <= span_start < end_idx:\n",
    "                start_word_index = i\n",
    "            if start_idx <= span_end <= end_idx:\n",
    "                end_word_index = i\n",
    "                break\n",
    "        return start_word_index, end_word_index\n",
    "\n",
    "    for span in spans:\n",
    "        start = span['start']\n",
    "        end = span['end']\n",
    "        tag = span['tag']\n",
    "        span_text = span['text']\n",
    "\n",
    "        start_word_index, end_word_index = find_word_indices(start, end)\n",
    "\n",
    "        # Define the surrounding words range\n",
    "        surrounding_start_index = max(0, start_word_index - 2)\n",
    "        surrounding_end_index = min(len(words_with_indices), end_word_index + 3)\n",
    "\n",
    "        # Extract surrounding words\n",
    "        surrounding_words = words_with_indices[surrounding_start_index:surrounding_end_index]\n",
    "        surrounding_text = ''.join(word for word, _, _ in surrounding_words)\n",
    "\n",
    "        new_span = {\n",
    "            \"tag\": tag,\n",
    "            \"text\": span_text,\n",
    "            \"surrounding\": surrounding_text.strip()\n",
    "        }\n",
    "\n",
    "        new_spans.append(new_span)\n",
    "\n",
    "    return new_spans\n",
    "\n",
    "\n",
    "\n",
    "def extract_tagged_text(abstract, tagged_text, tag_to_label):\n",
    "    '''\n",
    "    Function to extract the position of the tagged text from the abstract given its surrounding text\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    abstract : str\n",
    "        The abstract text\n",
    "    tagged_text : list of dict\n",
    "        The tagged text\n",
    "    tag_to_label : dict\n",
    "        Mapping of tags to labels\n",
    "    '''\n",
    "    output = []\n",
    "    \n",
    "    for entity in tagged_text:\n",
    "        surrounding_text = entity[\"surrounding\"]\n",
    "        entity_text = entity[\"text\"]\n",
    "        \n",
    "        # Handle special characters like non-breaking spaces\n",
    "        surrounding_text = surrounding_text.replace('\\xa0', ' ')\n",
    "        abstract = abstract.replace('\\xa0', ' ')\n",
    "        \n",
    "        # Find the start position of the surrounding text in the abstract\n",
    "        start_idx = abstract.lower().find(surrounding_text.lower())\n",
    "        \n",
    "        if start_idx != -1:\n",
    "            # Find the start position of the entity text within the surrounding text\n",
    "            surrounding_text_lower = surrounding_text.lower()\n",
    "            entity_text_lower = entity_text.lower()\n",
    "            \n",
    "            if entity_text.isdigit():\n",
    "                # Convert the number to words\n",
    "                entity_text_in_words = p.number_to_words(entity_text).lower()\n",
    "                # Check for both the number and its word representation\n",
    "                entity_start_in_surrounding = surrounding_text_lower.find(entity_text_lower)\n",
    "                found_form = \"numeric\"\n",
    "                if entity_start_in_surrounding == -1:\n",
    "                    entity_start_in_surrounding = surrounding_text_lower.find(entity_text_in_words)\n",
    "                    found_form = \"words\"\n",
    "            else:\n",
    "                entity_start_in_surrounding = surrounding_text_lower.find(entity_text_lower)\n",
    "                found_form = \"text\"\n",
    "            \n",
    "            if entity_start_in_surrounding != -1:\n",
    "                # Calculate the actual start and end positions in the abstract\n",
    "                actual_start_idx = start_idx + entity_start_in_surrounding\n",
    "                \n",
    "                if found_form == \"numeric\":\n",
    "                    actual_end_idx = actual_start_idx + len(entity_text)\n",
    "                elif found_form == \"words\":\n",
    "                    actual_end_idx = actual_start_idx + len(entity_text_in_words)\n",
    "                else:\n",
    "                    actual_end_idx = actual_start_idx + len(entity_text)\n",
    "                \n",
    "                output.append({\n",
    "                    \"start\": actual_start_idx,\n",
    "                    \"end\": actual_end_idx,\n",
    "                    \"label\": tag_to_label[entity[\"tag\"]],\n",
    "                    \"tag\": entity[\"tag\"],\n",
    "                    \"text\": entity[\"text\"]\n",
    "                })\n",
    "    \n",
    "    output.sort(key=lambda x: x[\"start\"])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_label = {\n",
    "    \"Disease/Condition of Interest\" : 8, #\n",
    "    \"Dosage\" : 9, #\n",
    "    \"Drug Intervention\" : 1 , #\n",
    "    \"Follow-up period\" : 16, # \n",
    "    \"Group Characteristic\" : 6, # \n",
    "    \"Group Name\" : 3,#  \n",
    "    \"Sample Size\" : 7, # \n",
    "    \"Intervention Administration\" : 13, # \n",
    "    \"Intervention Duration\" : 12, # \n",
    "    \"Intervention Frequency\" : 11, #  \n",
    "    \"Non-Pharmaceutical Intervention\" : 17, # \n",
    "    \"Non-Study Drug\" : 14, # \n",
    "    \"Outcome (Study Endpoint)\" : 2, # \n",
    "    \"Side Effects\" : 15, # Side effects\n",
    "    \"Quantitative Measurement\" : 0, # \n",
    "    \"Statistical Significance\" : 5, #\n",
    "    \"Study Duration\" : 19, # \n",
    "    \"Study Years\" : 18, #\n",
    "    \"Type of Quant. Measure\" : 10, # \n",
    "    \"Units\" : 4, #\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_prompts = {\n",
    "        \"Disease/Condition of Interest\": \"Return the disease/condition of interest. This is defined as what the drug aims to treat. Example usage: in 'metformin is being tested to treat PCOS', return 'PCOS'.\",\n",
    "        \"Drug Intervention\": \"Return the name(s) of the drug(s) tested. Don't highlight qualifying terms. Example usage: in 'high-dose-aspirin', return 'aspirin'.\",\n",
    "        \"Dosage\": \"Return the dosage of the drug(s) used in the study. Please return only the string of the numerical value used to convey the amount of drug given, excluding units. Example usage: in '2.5 mg Eliquis', return '2.5'. Example usage: in 'two grams', return 'two'.\",\n",
    "        \"Sample Size\": \"Return how many patients were enrolled in the study. Please give only the exact string and no other words. Do not convert words into numbers. For example, 'twenty-seven' stays as 'twenty-seven'. Example usage: in 'a total of 420 patients were enrolled in the study', return '420'. Example usage: in 'study had n=100', return '100'.\",\n",
    "        \"Follow-up period\": \"Return how long participants were tracked or examined after the initial intervention period. Example usage: in 'participants were followed for 1 year after intervention', return '1 year'.\",\n",
    "        \"Group Characteristic\": \"Return the trait(s) used to describe a group/groups of patients in the study. Example usage: in 'post-menopausal women were studied', return 'post-menopausal women.' Example usage: in 'participants with diastolic pressure > 80 mmHg...', return 'diastolic pressure > 80 mmHg'.\",\n",
    "        \"Group Name\": \"Return the group name(s) assigned to treatment or control group in the study Example usage: in 'first group had 20 mg (group A) and second group had 50 mg (group B)', return 'group A,group B'.\",\n",
    "        \"Intervention Administration\": \"Return the method in which the drug is provided to the patient/subject. Example usage: in '200 mg fluoxetine was given intravenously', return 'intravenously'. Other example matches include 'oral', 'inhaled', 'subcutaneous'.\",\n",
    "        \"Intervention Duration\": \"Return the amount of time the drug is taken/used. Example usage: in '2 g of aspirin was administered for 5 weeks', return '5 weeks'.\",\n",
    "        \"Intervention Frequency\": \"Return how often the drug taken. Example matches include 'B.I.D', 'daily', 'q8H'.\",\n",
    "        \"Non-Pharmaceutical Intervention\": \"If present, return a treatment that is not drug-related. This is an intervention being tested which is not a drug. Example usage: in 'patients undergoing hysterectomy were given 500 mg advil', return 'hysterectomy'.\",\n",
    "        \"Non-Study Drug\": \"If present, return additionally mentioned drugs that are not being mainly studied. This is a drug supplied or mentioned which is not of relevance to the studies' outcomes. Example usage: in 'we tested 87 asthmatic patients on Sertraline with Fluticasone', return 'Sertraline'. Example usage: in 'patients received a single-dose of metoprolol, supplied with intravenous saline for two weeks', return 'saline'.\",\n",
    "        \"Outcome (Study Endpoint)\": \"Return benchmarks that help evaluate the drug's efficacy or success, usually mentioned in the beginning and have quantitative support later in the abstract. The outcome(s) is\\/are what is being measured or assessed to relay the drugs effects on the condition/ disease of interest. Example usage: in 'There was a significant increase in heart rate in group 1 compared to group 2 (85% vs 10%, p = 0.001)', return 'heart rate'.\",\n",
    "        \"Side Effects\": \"If present, return side effects experienced by participants/subjects/patients while taking the study drug. Example usage: in 'Participants experienced fatigue, anxiety, etc. while taking metformin', return 'fatigue,anxiety'.\",\n",
    "        \"Quantitative Measurement\": \"Return numerical values that support the outcome and provide context for understanding. This value is measured in the study to evaluate the drug's effects on the outcomes. Only highlight the number, include parentheses if given as percentage, include +/-. Example usage: in 'There was a significant increase in heart rate in group 1 compared to group 2 (85% vs 10%, p = 0.001)', return '85%,10%'.\",\n",
    "        \"Statistical Significance\": \"Return statistical measurements used to describe quantitative data. Usually a p-value, return all p-values whether significant or nonsignificant. Example usage: in 'There was a significant increase in heart rate in group 1 compared to group 2 (85% vs 10%, p = 0.001)', return 'p = 0.001'.\",\n",
    "        \"Study Duration\": \"Return how long the study is. Example usage: in 'An 8-week, double-crossover, placebo-controlled, clinical trial...', return '8-week'.\",\n",
    "        \"Study Years\": \"Return the years the study takes place during. Example usage: in 'this was a 4 year study conducted from May 2013 to October 2017', return 'May 2013 to October 2017'. You should return the whole blurb (including the months).\",\n",
    "        \"Type of Quant. Measure\": \"Return the classification of a statistic. Example matches include 'hazard ratio', 'confidence interval', simple statistics like 'mean', 'median', 'odds ratio', etc. 'p' is not a type of quant measure. Example usage: in 'The mean (+- SD) in diastolic blood pressure was measured as 95 mmHg (Confidence Interval of 95%: 0.59 - 1.3)', return 'mean,SD,Confidence Interval of 95%'.\",\n",
    "        \"Units\": \"Return the unit of measurement used for a specific dosage or statistic. Example usage: in 'metformin 20 mg/kg/day for 6 months', return 'mg/kg'. If it comes after a number and it's not a true word, chances are it's a unit.\"\n",
    "    }\n",
    "\n",
    "format_spans = {\n",
    "    \"tag\": \"tag of the entity extracted\",\n",
    "    \"text\": \"entity extracted\",\n",
    "    \"surrounding\": \"surrounding of the text extracted\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_processed_for_modeling = \"../data/processed_for_modeling.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples for few-shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pmid4080 = 4080\n",
    "abstract4080, true_spans4080 = retrieve_abstract_and_spans(path_to_processed_for_modeling, pmid4080)\n",
    "spans4080 = create_spans_with_surrounding_text(text = abstract4080, spans = true_spans4080)\n",
    "\n",
    "pmid65221 = 65221\n",
    "abstract65221, true_spans65221 = retrieve_abstract_and_spans(path_to_processed_for_modeling, pmid65221)\n",
    "spans65221 = create_spans_with_surrounding_text(text = abstract65221, spans = true_spans65221)\n",
    "\n",
    "pmid29208464 = 29208464\n",
    "abstract29208464, true_spans29208464 = retrieve_abstract_and_spans(path_to_processed_for_modeling, 29208464)\n",
    "spans29208464 = create_spans_with_surrounding_text(text = abstract29208464, spans = true_spans29208464)\n",
    "\n",
    "pmid30872104 = 30872104\n",
    "abstract30872104, true_spans30872104 = retrieve_abstract_and_spans(path_to_processed_for_modeling, pmid30872104)\n",
    "spans30872104 = create_spans_with_surrounding_text(text = abstract30872104, spans = true_spans30872104)\n",
    "\n",
    "pmid35426326 = 35426326\n",
    "abstract35426326, true_spans35426326 = retrieve_abstract_and_spans(path_to_processed_for_modeling, pmid35426326)\n",
    "spans35426326 = create_spans_with_surrounding_text(text = abstract35426326, spans = true_spans35426326)\n",
    "\n",
    "pmid7484829 = 7484829\n",
    "abstract7484829, true_spans7484829 = retrieve_abstract_and_spans(path_to_processed_for_modeling, pmid7484829)\n",
    "spans7484829 = create_spans_with_surrounding_text(text = abstract7484829, spans = true_spans7484829)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 708, 'end': 712, 'label': 7, 'tag': 'Sample Size', 'text': 'four'},\n",
       " {'start': 752,\n",
       "  'end': 756,\n",
       "  'label': 1,\n",
       "  'tag': 'Drug Intervention',\n",
       "  'text': 'L-T4'},\n",
       " {'start': 811,\n",
       "  'end': 859,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'no significant difference between the two groups'},\n",
       " {'start': 881, 'end': 884, 'label': 3, 'tag': 'Group Name', 'text': 'SCH'},\n",
       " {'start': 889, 'end': 894, 'label': 3, 'tag': 'Group Name', 'text': 'TPOAb'},\n",
       " {'start': 929,\n",
       "  'end': 945,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'live births rate'},\n",
       " {'start': 975,\n",
       "  'end': 981,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'higher'},\n",
       " {'start': 1019,\n",
       "  'end': 1024,\n",
       "  'label': 0,\n",
       "  'tag': 'Quantitative Measurement',\n",
       "  'text': '79.5%'},\n",
       " {'start': 1029,\n",
       "  'end': 1034,\n",
       "  'label': 0,\n",
       "  'tag': 'Quantitative Measurement',\n",
       "  'text': '70.8%'},\n",
       " {'start': 1148,\n",
       "  'end': 1173,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'no significant difference'},\n",
       " {'start': 1303,\n",
       "  'end': 1308,\n",
       "  'label': 3,\n",
       "  'tag': 'Group Name',\n",
       "  'text': 'TPOAb'},\n",
       " {'start': 1330,\n",
       "  'end': 1334,\n",
       "  'label': 1,\n",
       "  'tag': 'Drug Intervention',\n",
       "  'text': 'L-T4'},\n",
       " {'start': 1456,\n",
       "  'end': 1481,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'no significant difference'},\n",
       " {'start': 1549,\n",
       "  'end': 1554,\n",
       "  'label': 3,\n",
       "  'tag': 'Group Name',\n",
       "  'text': 'TPOAb'},\n",
       " {'start': 1587,\n",
       "  'end': 1623,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'decreased the risk of pregnancy loss'},\n",
       " {'start': 1628,\n",
       "  'end': 1657,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'increased the live birth rate'},\n",
       " {'start': 1780,\n",
       "  'end': 1806,\n",
       "  'label': 10,\n",
       "  'tag': 'Type of Quant. Measure',\n",
       "  'text': 'SCH and TPOAb<sup>\\u2009+</sup>'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_spans35426326"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt gpt function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(abstract, model = \"gpt-3.5-turbo-0125\") : \n",
    "    completion = openai.chat.completions.create(\n",
    "        model = model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\" \n",
    "                Task : Looking at the following abstract: {abstract65221}.\\\n",
    "                Retrieve all labels that are in : {extraction_prompts}. \\\n",
    "                If the same entity appears multiple times in the text, it should be extracted each time it appears.\\\n",
    "                Answer : {spans65221}. \\\n",
    "                Task : Looking at the following abstract: {abstract4080}.\\\n",
    "                Retrieve all labels that are in : {extraction_prompts}. \\\n",
    "                If the same entity appears multiple times in the text, it should be extracted each time it appears.\\\n",
    "                Answer : {spans4080}. \\\n",
    "                Task : Looking at the following abstract: {abstract29208464}.\\\n",
    "                Retrieve all labels that are in : {extraction_prompts}. \\\n",
    "                If the same entity appears multiple times in the text, it should be extracted each time it appears.\\\n",
    "                Answer : {spans29208464}. \\\n",
    "                Task : Looking at the following abstract: {abstract30872104}.\\\n",
    "                Retrieve all labels that are in : {extraction_prompts}. \\\n",
    "                If the same entity appears multiple times in the text, it should be extracted each time it appears.\\\n",
    "                Answer : {spans30872104}. \\\n",
    "                Task : Looking at the following abstract: {abstract35426326}.\\\n",
    "                Retrieve all labels that are in : {extraction_prompts}. \\\n",
    "                If the same entity appears multiple times in the text, it should be extracted each time it appears.\\\n",
    "                Answer : {spans35426326}. \\\n",
    "                Task : Looking at the following abstract: {abstract}.\\\n",
    "                Retrieve all labels that are in : {extraction_prompts}. \\\n",
    "                If the same entity appears multiple times in the text, it should be extracted each time it appears. \\\n",
    "                Answer : \n",
    "                Entities must be extracted in this format : {format_spans}.\\\n",
    "                Return the answer in a JSON with \"entities\" as key and don’t output anything beyond the required JSON file.\n",
    "                \"\"\"\n",
    "            },\n",
    "        ],\n",
    "        max_tokens= 3072,\n",
    "        temperature= 0,\n",
    "    )\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seqeval evaluation library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_seqeval_format(y_true, y_pred, abstract):\n",
    "    '''\n",
    "    This function converts the entity data to seqeval format\n",
    "    Parameters:\n",
    "    ----------\n",
    "    y_true : list\n",
    "        Human annotated data\n",
    "    y_pred : list\n",
    "        Model predictions\n",
    "    abstract : str\n",
    "        The abstract to convert to seqeval format\n",
    "    '''\n",
    "    def label_tokens(annotations, abstract):\n",
    "        tokens = abstract.split()\n",
    "        labels = [\"O\"] * len(tokens)\n",
    "        for ann in annotations:\n",
    "            start_idx = len(abstract[:ann['start']].split())\n",
    "            end_idx = start_idx + len(ann['text'].split())\n",
    "            labels[start_idx] = f\"B-{ann['tag']}\"\n",
    "            for i in range(start_idx + 1, end_idx):\n",
    "                labels[i] = f\"I-{ann['tag']}\"\n",
    "        return labels\n",
    "    \n",
    "    y_true_seqeval = label_tokens(y_true, abstract)\n",
    "    y_pred_seqeval = label_tokens(y_pred, abstract)\n",
    "    return y_true_seqeval, y_pred_seqeval\n",
    "\n",
    "def label_tokens_from_offsets(text, annotations):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "\n",
    "    for ann in annotations:\n",
    "        start_char = ann['start']\n",
    "        end_char = ann['end']\n",
    "        start_token = next((i for i, token in enumerate(doc) if token.idx >= start_char), None)\n",
    "        end_token = next((i for i, token in enumerate(doc) if token.idx >= end_char), None)\n",
    "        \n",
    "        if start_token is not None and end_token is not None:\n",
    "            labels[start_token] = f\"B-{ann['tag']}\"\n",
    "            for i in range(start_token + 1, end_token):\n",
    "                labels[i] = f\"I-{ann['tag']}\"\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def compute_metrics(pmids, model=\"gpt-3.5-turbo-0125\", tag_to_label=None) :\n",
    "    all_y_true_seqeval = []\n",
    "    all_y_pred_seqeval = []\n",
    "    results_list = []\n",
    "    \n",
    "    for i in range(len(pmids)):\n",
    "        print(\"i :\", i)\n",
    "        print(\"pmid :\", pmids[i])\n",
    "        abstract, true_spans = retrieve_abstract_and_spans(path_to_processed_for_modeling = path_to_processed_for_modeling, pmid = pmids[i])\n",
    "        tagged_text = prompt(abstract, model)\n",
    "        if model == \"gpt-4o\" :\n",
    "            tagged_text = tagged_text.strip('```json').strip('```').strip()\n",
    "        tagged_text_json = json.loads(tagged_text)\n",
    "        pred_spans = extract_tagged_text(abstract=abstract, tagged_text=tagged_text_json[\"entities\"], tag_to_label=tag_to_label)\n",
    "        y_true_seqeval = label_tokens_from_offsets(text = abstract, annotations = true_spans)\n",
    "        y_pred_seqeval = label_tokens_from_offsets(text = abstract, annotations = pred_spans)\n",
    "        all_y_true_seqeval.append(y_true_seqeval)\n",
    "        all_y_pred_seqeval.append(y_pred_seqeval)\n",
    "        results = seqeval.compute(predictions=[y_pred_seqeval], references=[y_true_seqeval])\n",
    "        results['pmid'] = pmids[i]\n",
    "        results['model'] = model\n",
    "        results_list.append(results)\n",
    "\n",
    "        \n",
    "    # Evaluate using seqeval\n",
    "    overall_results = seqeval.compute(predictions=all_y_pred_seqeval, references=all_y_true_seqeval)\n",
    "    overall_class_specific_f1 = {\n",
    "        k: v[\"f1\"] for k, v in overall_results.items() if not k.startswith(\"overall\")\n",
    "    }\n",
    "\n",
    "    \n",
    "    df_results = pd.DataFrame(results_list)\n",
    "    columns = ['pmid', 'model', 'overall_accuracy', 'overall_precision', 'overall_recall', 'overall_f1'] # + \\\n",
    "                    # [col for col in df_results.columns if col not in ['pmid', 'overall_accuracy', 'overall_precision', 'overall_recall', 'overall_f1']]\n",
    "    df_results = df_results[columns].rename(columns={\n",
    "        'overall_accuracy': 'accuracy',\n",
    "        'overall_precision': 'precision',\n",
    "        'overall_recall': 'recall',\n",
    "        'overall_f1': 'f1'\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": overall_results[\"overall_accuracy\"],\n",
    "        \"precision\": overall_results[\"overall_precision\"],\n",
    "        \"recall\": overall_results[\"overall_recall\"],\n",
    "        \"f1\": overall_results[\"overall_f1\"],\n",
    "        \"class_specific_f1\": overall_class_specific_f1,\n",
    "        \"detailed_results\": overall_results,\n",
    "    }, df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_processed_for_modeling, \"r\") as file:\n",
    "    data = ujson.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [item for item in data if item[\"split\"] == \"test\"]\n",
    "pmids_test = [item[\"pmid\"] for item in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[58651]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmids_test = pmids_test[:1]\n",
    "pmids_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i : 0\n",
      "pmid : 58651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "/nethome/cye73/conda_envs/trialsieve/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/nethome/cye73/conda_envs/trialsieve/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7838983050847458,\n",
       " 'precision': np.float64(0.5416666666666666),\n",
       " 'recall': np.float64(0.38235294117647056),\n",
       " 'f1': np.float64(0.44827586206896547),\n",
       " 'class_specific_f1': {'Disease/Condition of Interest': np.float64(0.0),\n",
       "  'Drug Intervention': np.float64(0.25),\n",
       "  'Follow-up period': np.float64(0.0),\n",
       "  'Group Characteristic': np.float64(1.0),\n",
       "  'Group Name': np.float64(0.0),\n",
       "  'Intervention Administration': np.float64(0.0),\n",
       "  'Non-Study Drug': np.float64(0.0),\n",
       "  'Outcome (Study Endpoint)': np.float64(0.0),\n",
       "  'Quantitative Measurement': np.float64(0.8),\n",
       "  'Sample Size': np.float64(0.7142857142857143),\n",
       "  'Statistical Significance': np.float64(0.6666666666666666)},\n",
       " 'detailed_results': {'Disease/Condition of Interest': {'precision': np.float64(0.0),\n",
       "   'recall': np.float64(0.0),\n",
       "   'f1': np.float64(0.0),\n",
       "   'number': np.int64(0)},\n",
       "  'Drug Intervention': {'precision': np.float64(0.25),\n",
       "   'recall': np.float64(0.25),\n",
       "   'f1': np.float64(0.25),\n",
       "   'number': np.int64(4)},\n",
       "  'Follow-up period': {'precision': np.float64(0.0),\n",
       "   'recall': np.float64(0.0),\n",
       "   'f1': np.float64(0.0),\n",
       "   'number': np.int64(0)},\n",
       "  'Group Characteristic': {'precision': np.float64(1.0),\n",
       "   'recall': np.float64(1.0),\n",
       "   'f1': np.float64(1.0),\n",
       "   'number': np.int64(1)},\n",
       "  'Group Name': {'precision': np.float64(0.0),\n",
       "   'recall': np.float64(0.0),\n",
       "   'f1': np.float64(0.0),\n",
       "   'number': np.int64(8)},\n",
       "  'Intervention Administration': {'precision': np.float64(0.0),\n",
       "   'recall': np.float64(0.0),\n",
       "   'f1': np.float64(0.0),\n",
       "   'number': np.int64(0)},\n",
       "  'Non-Study Drug': {'precision': np.float64(0.0),\n",
       "   'recall': np.float64(0.0),\n",
       "   'f1': np.float64(0.0),\n",
       "   'number': np.int64(2)},\n",
       "  'Outcome (Study Endpoint)': {'precision': np.float64(0.0),\n",
       "   'recall': np.float64(0.0),\n",
       "   'f1': np.float64(0.0),\n",
       "   'number': np.int64(4)},\n",
       "  'Quantitative Measurement': {'precision': np.float64(0.6666666666666666),\n",
       "   'recall': np.float64(1.0),\n",
       "   'f1': np.float64(0.8),\n",
       "   'number': np.int64(4)},\n",
       "  'Sample Size': {'precision': np.float64(0.7142857142857143),\n",
       "   'recall': np.float64(0.7142857142857143),\n",
       "   'f1': np.float64(0.7142857142857143),\n",
       "   'number': np.int64(7)},\n",
       "  'Statistical Significance': {'precision': np.float64(1.0),\n",
       "   'recall': np.float64(0.5),\n",
       "   'f1': np.float64(0.6666666666666666),\n",
       "   'number': np.int64(4)},\n",
       "  'overall_precision': np.float64(0.5416666666666666),\n",
       "  'overall_recall': np.float64(0.38235294117647056),\n",
       "  'overall_f1': np.float64(0.44827586206896547),\n",
       "  'overall_accuracy': 0.7838983050847458}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, df = compute_metrics(pmids = pmids_test, \n",
    "                                 model=\"gpt-4o\",\n",
    "                                 tag_to_label=tag_to_label\n",
    "                                 )\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58651</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>0.783898</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.448276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pmid   model  accuracy  precision    recall        f1\n",
       "0  58651  gpt-4o  0.783898   0.541667  0.382353  0.448276"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trialsieve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
