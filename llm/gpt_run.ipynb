{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/cye73/conda_envs/bioel/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "from ids import open_ai_api_key\n",
    "from thefuzz import fuzz, process\n",
    "import re\n",
    "import ujson\n",
    "import logging\n",
    "import inflect\n",
    "from collections import Counter, defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import evaluate\n",
    "import spacy\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "openai.api_key = open_ai_api_key\n",
    "\n",
    "# seqeval evaluation\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "# spacy tokenizer\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create an engine object\n",
    "p = inflect.engine()\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2 : Few-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_abstract_and_spans(path_to_processed_for_modeling, pmid):\n",
    "    ''' \n",
    "    Function to retrieve the abstract and spans from the processed_for_modeling.json file\n",
    "    ----------\n",
    "    path_to_processed_for_modeling : str\n",
    "        The path to the processed_for_modeling.json file\n",
    "    pmid : int\n",
    "        The pmid of the article to retrieve the spans\n",
    "    '''\n",
    "    with open(path_to_processed_for_modeling, \"r\") as file:\n",
    "        processed_for_modeling = ujson.load(file)\n",
    "    spans = None\n",
    "    for article in processed_for_modeling:\n",
    "        if article[\"pmid\"] == pmid:\n",
    "            abstract = article[\"text\"]\n",
    "            spans = article[\"spans\"]\n",
    "            break\n",
    "    if spans is None:\n",
    "        logging.info(\"pmid not found in the list of documents\")\n",
    "    \n",
    "    return abstract, spans\n",
    "\n",
    "    \n",
    "def tagged_abstract(path_to_processed_for_modeling, pmid):\n",
    "    '''\n",
    "    Function to retrieve the tagged abstract from the processed_for_modeling.json file\n",
    "    ----------\n",
    "    path_to_processed_for_modeling : str\n",
    "        The path to the processed_for_modeling.json file\n",
    "    pmid : int\n",
    "        The pmid of the article to retrieve the spans\n",
    "    '''\n",
    "    with open(path_to_processed_for_modeling, \"r\") as file:\n",
    "        processed_for_modeling = ujson.load(file)\n",
    "    tagged_abstract = defaultdict(list)\n",
    "    for article in processed_for_modeling:\n",
    "        if article[\"pmid\"] == pmid:\n",
    "            for span in article[\"spans\"]:\n",
    "                tagged_abstract[span[\"tag\"]].append(span[\"text\"])\n",
    "    return tagged_abstract\n",
    "\n",
    "\n",
    "def create_spans_with_surrounding_text(text, spans):\n",
    "    '''\n",
    "    This function creates a new list of spans with the surrounding text (+- 1 word) of each span\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    text : str\n",
    "        The abstract text\n",
    "    spans : list of dict\n",
    "        The spans extracted from the text : start, end, label, tag and text\n",
    "    '''\n",
    "    new_spans = []\n",
    "    # Use regex to split the text into words, including punctuation as separate tokens\n",
    "    words_with_indices = [(m.group(0), m.start(), m.end()) for m in re.finditer(r'\\S+|\\s+', text)]\n",
    "\n",
    "    def find_word_indices(span_start, span_end):\n",
    "        start_word_index = 0\n",
    "        end_word_index = 0\n",
    "        for i, (word, start_idx, end_idx) in enumerate(words_with_indices):\n",
    "            if start_idx <= span_start < end_idx:\n",
    "                start_word_index = i\n",
    "            if start_idx <= span_end <= end_idx:\n",
    "                end_word_index = i\n",
    "                break\n",
    "        return start_word_index, end_word_index\n",
    "\n",
    "    for span in spans:\n",
    "        start = span['start']\n",
    "        end = span['end']\n",
    "        tag = span['tag']\n",
    "        span_text = span['text']\n",
    "\n",
    "        start_word_index, end_word_index = find_word_indices(start, end)\n",
    "\n",
    "        # Define the surrounding words range\n",
    "        surrounding_start_index = max(0, start_word_index - 2)\n",
    "        surrounding_end_index = min(len(words_with_indices), end_word_index + 3)\n",
    "\n",
    "        # Extract surrounding words\n",
    "        surrounding_words = words_with_indices[surrounding_start_index:surrounding_end_index]\n",
    "        surrounding_text = ''.join(word for word, _, _ in surrounding_words)\n",
    "\n",
    "        new_span = {\n",
    "            \"tag\": tag,\n",
    "            \"text\": span_text,\n",
    "            \"surrounding\": surrounding_text.strip()\n",
    "        }\n",
    "\n",
    "        new_spans.append(new_span)\n",
    "\n",
    "    return new_spans\n",
    "\n",
    "\n",
    "\n",
    "def extract_tagged_text(abstract, tagged_text, tag_to_label):\n",
    "    '''\n",
    "    Function to extract the position of the tagged text from the abstract given its surrounding text\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    abstract : str\n",
    "        The abstract text\n",
    "    tagged_text : list of dict\n",
    "        The tagged text\n",
    "    tag_to_label : dict\n",
    "        Mapping of tags to labels\n",
    "    '''\n",
    "    output = []\n",
    "    \n",
    "    for entity in tagged_text:\n",
    "        surrounding_text = entity[\"surrounding\"]\n",
    "        entity_text = entity[\"text\"]\n",
    "        \n",
    "        # Handle special characters like non-breaking spaces\n",
    "        surrounding_text = surrounding_text.replace('\\xa0', ' ')\n",
    "        abstract = abstract.replace('\\xa0', ' ')\n",
    "        \n",
    "        # Find the start position of the surrounding text in the abstract\n",
    "        start_idx = abstract.lower().find(surrounding_text.lower())\n",
    "        \n",
    "        if start_idx != -1:\n",
    "            # Find the start position of the entity text within the surrounding text\n",
    "            surrounding_text_lower = surrounding_text.lower()\n",
    "            entity_text_lower = entity_text.lower()\n",
    "            \n",
    "            if entity_text.isdigit():\n",
    "                # Convert the number to words\n",
    "                entity_text_in_words = p.number_to_words(entity_text).lower()\n",
    "                # Check for both the number and its word representation\n",
    "                entity_start_in_surrounding = surrounding_text_lower.find(entity_text_lower)\n",
    "                found_form = \"numeric\"\n",
    "                if entity_start_in_surrounding == -1:\n",
    "                    entity_start_in_surrounding = surrounding_text_lower.find(entity_text_in_words)\n",
    "                    found_form = \"words\"\n",
    "            else:\n",
    "                entity_start_in_surrounding = surrounding_text_lower.find(entity_text_lower)\n",
    "                found_form = \"text\"\n",
    "            \n",
    "            if entity_start_in_surrounding != -1:\n",
    "                # Calculate the actual start and end positions in the abstract\n",
    "                actual_start_idx = start_idx + entity_start_in_surrounding\n",
    "                \n",
    "                if found_form == \"numeric\":\n",
    "                    actual_end_idx = actual_start_idx + len(entity_text)\n",
    "                elif found_form == \"words\":\n",
    "                    actual_end_idx = actual_start_idx + len(entity_text_in_words)\n",
    "                else:\n",
    "                    actual_end_idx = actual_start_idx + len(entity_text)\n",
    "                \n",
    "                output.append({\n",
    "                    \"start\": actual_start_idx,\n",
    "                    \"end\": actual_end_idx,\n",
    "                    \"label\": tag_to_label[entity[\"tag\"]],\n",
    "                    \"tag\": entity[\"tag\"],\n",
    "                    \"text\": entity[\"text\"]\n",
    "                })\n",
    "    \n",
    "    output.sort(key=lambda x: x[\"start\"])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_label = {\n",
    "    \"Disease/Condition of Interest\" : 8, #\n",
    "    \"Dosage\" : 9, #\n",
    "    \"Drug Intervention\" : 1 , #\n",
    "    \"Follow-up period\" : 16, # \n",
    "    \"Group Characteristic\" : 6, # \n",
    "    \"Group Name\" : 3,#  \n",
    "    \"Sample Size\" : 7, # \n",
    "    \"Intervention Administration\" : 13, # \n",
    "    \"Intervention Duration\" : 12, # \n",
    "    \"Intervention Frequency\" : 11, #  \n",
    "    \"Non-Pharmaceutical Intervention\" : 17, # \n",
    "    \"Non-Study Drug\" : 14, # \n",
    "    \"Outcome (Study Endpoint)\" : 2, # \n",
    "    \"Side Effects\" : 15, # Side effects\n",
    "    \"Quantitative Measurement\" : 0, # \n",
    "    \"Statistical Significance\" : 5, #\n",
    "    \"Study Duration\" : 19, # \n",
    "    \"Study Years\" : 18, #\n",
    "    \"Type of Quant. Measure\" : 10, # \n",
    "    \"Units\" : 4, #\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_prompts = {\n",
    "        \"Disease/Condition of Interest\": \"Return the disease/condition of interest. This is defined as what the drug aims to treat. Example usage: in 'metformin is being tested to treat PCOS', return 'PCOS'.\",\n",
    "        \"Drug Intervention\": \"Return the name(s) of the drug(s) tested. Don't highlight qualifying terms. Example usage: in 'high-dose-aspirin', return 'aspirin'.\",\n",
    "        \"Dosage\": \"Return the dosage of the drug(s) used in the study. Please return only the string of the numerical value used to convey the amount of drug given, excluding units. Example usage: in '2.5 mg Eliquis', return '2.5'. Example usage: in 'two grams', return 'two'.\",\n",
    "        \"Sample Size\": \"Return how many patients were enrolled in the study. Please give only the exact string and no other words. Do not convert words into numbers. For example, 'twenty-seven' stays as 'twenty-seven'. Example usage: in 'a total of 420 patients were enrolled in the study', return '420'. Example usage: in 'study had n=100', return '100'.\",\n",
    "        \"Follow-up period\": \"Return how long participants were tracked or examined after the initial intervention period. Example usage: in 'participants were followed for 1 year after intervention', return '1 year'.\",\n",
    "        \"Group Characteristic\": \"Return the trait(s) used to describe a group/groups of patients in the study. Example usage: in 'post-menopausal women were studied', return 'post-menopausal women.' Example usage: in 'participants with diastolic pressure > 80 mmHg...', return 'diastolic pressure > 80 mmHg'.\",\n",
    "        \"Group Name\": \"Return the group name(s) assigned to treatment or control group in the study Example usage: in 'first group had 20 mg (group A) and second group had 50 mg (group B)', return 'group A,group B'.\",\n",
    "        \"Intervention Administration\": \"Return the method in which the drug is provided to the patient/subject. Example usage: in '200 mg fluoxetine was given intravenously', return 'intravenously'. Other example matches include 'oral', 'inhaled', 'subcutaneous'.\",\n",
    "        \"Intervention Duration\": \"Return the amount of time the drug is taken/used. Example usage: in '2 g of aspirin was administered for 5 weeks', return '5 weeks'.\",\n",
    "        \"Intervention Frequency\": \"Return how often the drug taken. Example matches include 'B.I.D', 'daily', 'q8H'.\",\n",
    "        \"Non-Pharmaceutical Intervention\": \"If present, return a treatment that is not drug-related. This is an intervention being tested which is not a drug. Example usage: in 'patients undergoing hysterectomy were given 500 mg advil', return 'hysterectomy'.\",\n",
    "        \"Non-Study Drug\": \"If present, return additionally mentioned drugs that are not being mainly studied. This is a drug supplied or mentioned which is not of relevance to the studies' outcomes. Example usage: in 'we tested 87 asthmatic patients on Sertraline with Fluticasone', return 'Sertraline'. Example usage: in 'patients received a single-dose of metoprolol, supplied with intravenous saline for two weeks', return 'saline'.\",\n",
    "        \"Outcome (Study Endpoint)\": \"Return benchmarks that help evaluate the drug's efficacy or success, usually mentioned in the beginning and have quantitative support later in the abstract. The outcome(s) is\\/are what is being measured or assessed to relay the drugs effects on the condition/ disease of interest. Example usage: in 'There was a significant increase in heart rate in group 1 compared to group 2 (85% vs 10%, p = 0.001)', return 'heart rate'.\",\n",
    "        \"Side Effects\": \"If present, return side effects experienced by participants/subjects/patients while taking the study drug. Example usage: in 'Participants experienced fatigue, anxiety, etc. while taking metformin', return 'fatigue,anxiety'.\",\n",
    "        \"Quantitative Measurement\": \"Return numerical values that support the outcome and provide context for understanding. This value is measured in the study to evaluate the drug's effects on the outcomes. Only highlight the number, include parentheses if given as percentage, include +/-. Example usage: in 'There was a significant increase in heart rate in group 1 compared to group 2 (85% vs 10%, p = 0.001)', return '85%,10%'.\",\n",
    "        \"Statistical Significance\": \"Return statistical measurements used to describe quantitative data. Usually a p-value, return all p-values whether significant or nonsignificant. Example usage: in 'There was a significant increase in heart rate in group 1 compared to group 2 (85% vs 10%, p = 0.001)', return 'p = 0.001'.\",\n",
    "        \"Study Duration\": \"Return how long the study is. Example usage: in 'An 8-week, double-crossover, placebo-controlled, clinical trial...', return '8-week'.\",\n",
    "        \"Study Years\": \"Return the years the study takes place during. Example usage: in 'this was a 4 year study conducted from May 2013 to October 2017', return 'May 2013 to October 2017'. You should return the whole blurb (including the months).\",\n",
    "        \"Type of Quant. Measure\": \"Return the classification of a statistic. Example matches include 'hazard ratio', 'confidence interval', simple statistics like 'mean', 'median', 'odds ratio', etc. 'p' is not a type of quant measure. Example usage: in 'The mean (+- SD) in diastolic blood pressure was measured as 95 mmHg (Confidence Interval of 95%: 0.59 - 1.3)', return 'mean,SD,Confidence Interval of 95%'.\",\n",
    "        \"Units\": \"Return the unit of measurement used for a specific dosage or statistic. Example usage: in 'metformin 20 mg/kg/day for 6 months', return 'mg/kg'. If it comes after a number and it's not a true word, chances are it's a unit.\"\n",
    "    }\n",
    "\n",
    "format_spans = {\n",
    "    \"tag\": \"tag of the entity extracted\",\n",
    "    \"text\": \"entity extracted\",\n",
    "    \"surrounding\": \"surrounding of the text extracted\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_processed_for_modeling = \"/home2/cye73/gpt-meta-analysis/tests/processed_for_modeling.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pmid4080 = 4080\n",
    "abstract4080, true_spans4080 = retrieve_abstract_and_spans(path_to_processed_for_modeling, pmid4080)\n",
    "spans4080 = create_spans_with_surrounding_text(text = abstract4080, spans = true_spans4080)\n",
    "\n",
    "pmid65221 = 65221\n",
    "abstract65221, true_spans65221 = retrieve_abstract_and_spans(path_to_processed_for_modeling, pmid65221)\n",
    "spans65221 = create_spans_with_surrounding_text(text = abstract65221, spans = true_spans65221)\n",
    "\n",
    "pmid29208464 = 29208464\n",
    "abstract29208464, true_spans29208464 = retrieve_abstract_and_spans(path_to_processed_for_modeling, 29208464)\n",
    "spans29208464 = create_spans_with_surrounding_text(text = abstract29208464, spans = true_spans29208464)\n",
    "\n",
    "pmid30872104 = 30872104\n",
    "abstract30872104, true_spans30872104 = retrieve_abstract_and_spans(path_to_processed_for_modeling, pmid30872104)\n",
    "spans30872104 = create_spans_with_surrounding_text(text = abstract30872104, spans = true_spans30872104)\n",
    "\n",
    "pmid35426326 = 35426326\n",
    "abstract35426326, true_spans35426326 = retrieve_abstract_and_spans(path_to_processed_for_modeling, pmid35426326)\n",
    "spans35426326 = create_spans_with_surrounding_text(text = abstract35426326, spans = true_spans35426326)\n",
    "\n",
    "pmid7484829 = 7484829\n",
    "abstract7484829, true_spans7484829 = retrieve_abstract_and_spans(path_to_processed_for_modeling, pmid7484829)\n",
    "spans7484829 = create_spans_with_surrounding_text(text = abstract7484829, spans = true_spans7484829)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 708, 'end': 712, 'label': 7, 'tag': 'Sample Size', 'text': 'four'},\n",
       " {'start': 752,\n",
       "  'end': 756,\n",
       "  'label': 1,\n",
       "  'tag': 'Drug Intervention',\n",
       "  'text': 'L-T4'},\n",
       " {'start': 811,\n",
       "  'end': 859,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'no significant difference between the two groups'},\n",
       " {'start': 881, 'end': 884, 'label': 3, 'tag': 'Group Name', 'text': 'SCH'},\n",
       " {'start': 889, 'end': 894, 'label': 3, 'tag': 'Group Name', 'text': 'TPOAb'},\n",
       " {'start': 929,\n",
       "  'end': 945,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'live births rate'},\n",
       " {'start': 975,\n",
       "  'end': 981,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'higher'},\n",
       " {'start': 1019,\n",
       "  'end': 1024,\n",
       "  'label': 0,\n",
       "  'tag': 'Quantitative Measurement',\n",
       "  'text': '79.5%'},\n",
       " {'start': 1029,\n",
       "  'end': 1034,\n",
       "  'label': 0,\n",
       "  'tag': 'Quantitative Measurement',\n",
       "  'text': '70.8%'},\n",
       " {'start': 1148,\n",
       "  'end': 1173,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'no significant difference'},\n",
       " {'start': 1303,\n",
       "  'end': 1308,\n",
       "  'label': 3,\n",
       "  'tag': 'Group Name',\n",
       "  'text': 'TPOAb'},\n",
       " {'start': 1330,\n",
       "  'end': 1334,\n",
       "  'label': 1,\n",
       "  'tag': 'Drug Intervention',\n",
       "  'text': 'L-T4'},\n",
       " {'start': 1456,\n",
       "  'end': 1481,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'no significant difference'},\n",
       " {'start': 1549,\n",
       "  'end': 1554,\n",
       "  'label': 3,\n",
       "  'tag': 'Group Name',\n",
       "  'text': 'TPOAb'},\n",
       " {'start': 1587,\n",
       "  'end': 1623,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'decreased the risk of pregnancy loss'},\n",
       " {'start': 1628,\n",
       "  'end': 1657,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'increased the live birth rate'},\n",
       " {'start': 1780,\n",
       "  'end': 1806,\n",
       "  'label': 10,\n",
       "  'tag': 'Type of Quant. Measure',\n",
       "  'text': 'SCH and TPOAb<sup>\\u2009+</sup>'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_spans35426326"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(abstract, model = \"gpt-3.5-turbo-0125\") : \n",
    "    completion = openai.chat.completions.create(\n",
    "        model = model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\" \n",
    "                Task : Looking at the following abstract: {abstract65221}.\\\n",
    "                Retrieve all labels that are in : {extraction_prompts}. \\\n",
    "                If the same entity appears multiple times in the text, it should be extracted each time it appears.\\\n",
    "                Answer : {spans65221}. \\\n",
    "                Task : Looking at the following abstract: {abstract4080}.\\\n",
    "                Retrieve all labels that are in : {extraction_prompts}. \\\n",
    "                If the same entity appears multiple times in the text, it should be extracted each time it appears.\\\n",
    "                Answer : {spans4080}. \\\n",
    "                Task : Looking at the following abstract: {abstract29208464}.\\\n",
    "                Retrieve all labels that are in : {extraction_prompts}. \\\n",
    "                If the same entity appears multiple times in the text, it should be extracted each time it appears.\\\n",
    "                Answer : {spans29208464}. \\\n",
    "                Task : Looking at the following abstract: {abstract30872104}.\\\n",
    "                Retrieve all labels that are in : {extraction_prompts}. \\\n",
    "                If the same entity appears multiple times in the text, it should be extracted each time it appears.\\\n",
    "                Answer : {spans30872104}. \\\n",
    "                Task : Looking at the following abstract: {abstract35426326}.\\\n",
    "                Retrieve all labels that are in : {extraction_prompts}. \\\n",
    "                If the same entity appears multiple times in the text, it should be extracted each time it appears.\\\n",
    "                Answer : {spans35426326}. \\\n",
    "                Task : Looking at the following abstract: {abstract}.\\\n",
    "                Retrieve all labels that are in : {extraction_prompts}. \\\n",
    "                If the same entity appears multiple times in the text, it should be extracted each time it appears. \\\n",
    "                Answer : \n",
    "                Entities must be extracted in this format : {format_spans}.\\\n",
    "                Return the answer in a JSON with \"entities\" as key and don’t output anything beyond the required JSON file.\n",
    "                \"\"\"\n",
    "            },\n",
    "        ],\n",
    "        max_tokens= 3072,\n",
    "        temperature= 0,\n",
    "    )\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seqeval evaluation library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_seqeval_format(y_true, y_pred, abstract):\n",
    "    '''\n",
    "    This function converts the entity data to seqeval format\n",
    "    Parameters:\n",
    "    ----------\n",
    "    y_true : list\n",
    "        Human annotated data\n",
    "    y_pred : list\n",
    "        Model predictions\n",
    "    abstract : str\n",
    "        The abstract to convert to seqeval format\n",
    "    '''\n",
    "    def label_tokens(annotations, abstract):\n",
    "        tokens = abstract.split()\n",
    "        labels = [\"O\"] * len(tokens)\n",
    "        for ann in annotations:\n",
    "            start_idx = len(abstract[:ann['start']].split())\n",
    "            end_idx = start_idx + len(ann['text'].split())\n",
    "            labels[start_idx] = f\"B-{ann['tag']}\"\n",
    "            for i in range(start_idx + 1, end_idx):\n",
    "                labels[i] = f\"I-{ann['tag']}\"\n",
    "        return labels\n",
    "    \n",
    "    y_true_seqeval = label_tokens(y_true, abstract)\n",
    "    y_pred_seqeval = label_tokens(y_pred, abstract)\n",
    "    return y_true_seqeval, y_pred_seqeval\n",
    "\n",
    "def label_tokens_from_offsets(text, annotations):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "\n",
    "    for ann in annotations:\n",
    "        start_char = ann['start']\n",
    "        end_char = ann['end']\n",
    "        start_token = next((i for i, token in enumerate(doc) if token.idx >= start_char), None)\n",
    "        end_token = next((i for i, token in enumerate(doc) if token.idx >= end_char), None)\n",
    "        \n",
    "        if start_token is not None and end_token is not None:\n",
    "            labels[start_token] = f\"B-{ann['tag']}\"\n",
    "            for i in range(start_token + 1, end_token):\n",
    "                labels[i] = f\"I-{ann['tag']}\"\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def compute_metrics_v2(pmids, model=\"gpt-3.5-turbo-0125\") :\n",
    "    all_y_true_seqeval = []\n",
    "    all_y_pred_seqeval = []\n",
    "    results_list = []\n",
    "    \n",
    "    for i in range(len(pmids)):\n",
    "        print(\"i :\", i)\n",
    "        print(\"pmid :\", pmids[i])\n",
    "        abstract, true_spans = retrieve_abstract_and_spans(path_to_processed_for_modeling = path_to_processed_for_modeling, pmid = pmids[i])\n",
    "        tagged_text = prompt(abstract, model)\n",
    "        if model == \"gpt-4o\" :\n",
    "            tagged_text = tagged_text.strip('```json').strip('```').strip()\n",
    "        tagged_text_json = json.loads(tagged_text)\n",
    "        pred_spans = extract_tagged_text(abstract=abstract, tagged_text=tagged_text_json[\"entities\"])\n",
    "        y_true_seqeval = label_tokens_from_offsets(text = abstract, annotations = true_spans)\n",
    "        y_pred_seqeval = label_tokens_from_offsets(text = abstract, annotations = pred_spans)\n",
    "        all_y_true_seqeval.append(y_true_seqeval)\n",
    "        all_y_pred_seqeval.append(y_pred_seqeval)\n",
    "        results = seqeval.compute(predictions=[y_pred_seqeval], references=[y_true_seqeval])\n",
    "        results['pmid'] = pmids[i]\n",
    "        results['model'] = model\n",
    "        results_list.append(results)\n",
    "\n",
    "        \n",
    "    # Evaluate using seqeval\n",
    "    overall_results = seqeval.compute(predictions=all_y_pred_seqeval, references=all_y_true_seqeval)\n",
    "    overall_class_specific_f1 = {\n",
    "        k: v[\"f1\"] for k, v in overall_results.items() if not k.startswith(\"overall\")\n",
    "    }\n",
    "\n",
    "    \n",
    "    df_results = pd.DataFrame(results_list)\n",
    "    columns = ['pmid', 'model', 'overall_accuracy', 'overall_precision', 'overall_recall', 'overall_f1'] # + \\\n",
    "                    # [col for col in df_results.columns if col not in ['pmid', 'overall_accuracy', 'overall_precision', 'overall_recall', 'overall_f1']]\n",
    "    df_results = df_results[columns].rename(columns={\n",
    "        'overall_accuracy': 'accuracy',\n",
    "        'overall_precision': 'precision',\n",
    "        'overall_recall': 'recall',\n",
    "        'overall_f1': 'f1'\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": overall_results[\"overall_accuracy\"],\n",
    "        \"precision\": overall_results[\"overall_precision\"],\n",
    "        \"recall\": overall_results[\"overall_recall\"],\n",
    "        \"f1\": overall_results[\"overall_f1\"],\n",
    "        \"class_specific_f1\": overall_class_specific_f1,\n",
    "        \"detailed_results\": overall_results,\n",
    "    }, df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_processed_for_modeling, \"r\") as file:\n",
    "    data = ujson.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [item for item in data if item[\"split\"] == \"test\"]\n",
    "pmids_test = [item[\"pmid\"] for item in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[58651, 527347, 779588, 1366257, 1703608]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmids_test = pmids_test[:5]\n",
    "pmids_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i : 0\n",
      "pmid : 58651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "/nethome/cye73/conda_envs/bioel/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i : 1\n",
      "pmid : 527347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "/nethome/cye73/conda_envs/bioel/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i : 2\n",
      "pmid : 779588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i : 3\n",
      "pmid : 1366257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i : 4\n",
      "pmid : 1703608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8471683475562451,\n",
       " 'precision': 0.7480916030534351,\n",
       " 'recall': 0.5833333333333334,\n",
       " 'f1': 0.6555183946488294,\n",
       " 'class_specific_f1': {'Disease/Condition of Interest': 0.6,\n",
       "  'Dosage': 0.8461538461538461,\n",
       "  'Drug Intervention': 0.4848484848484849,\n",
       "  'Group Characteristic': 0.28571428571428575,\n",
       "  'Group Name': 0.6399999999999999,\n",
       "  'Intervention Administration': 0.6666666666666666,\n",
       "  'Intervention Duration': 0.5714285714285715,\n",
       "  'Intervention Frequency': 0.23529411764705885,\n",
       "  'Non-Pharmaceutical Intervention': 0.0,\n",
       "  'Non-Study Drug': 0.5714285714285715,\n",
       "  'Outcome (Study Endpoint)': 0.6250000000000001,\n",
       "  'Quantitative Measurement': 0.6666666666666667,\n",
       "  'Sample Size': 0.8148148148148148,\n",
       "  'Side Effects': 0.0,\n",
       "  'Statistical Significance': 1.0,\n",
       "  'Study Duration': 0.0,\n",
       "  'Type of Quant. Measure': 0.8,\n",
       "  'Units': 0.8275862068965517},\n",
       " 'detailed_results': {'Disease/Condition of Interest': {'precision': 0.75,\n",
       "   'recall': 0.5,\n",
       "   'f1': 0.6,\n",
       "   'number': 6},\n",
       "  'Dosage': {'precision': 0.8461538461538461,\n",
       "   'recall': 0.8461538461538461,\n",
       "   'f1': 0.8461538461538461,\n",
       "   'number': 13},\n",
       "  'Drug Intervention': {'precision': 0.6666666666666666,\n",
       "   'recall': 0.38095238095238093,\n",
       "   'f1': 0.4848484848484849,\n",
       "   'number': 21},\n",
       "  'Group Characteristic': {'precision': 0.3333333333333333,\n",
       "   'recall': 0.25,\n",
       "   'f1': 0.28571428571428575,\n",
       "   'number': 4},\n",
       "  'Group Name': {'precision': 1.0,\n",
       "   'recall': 0.47058823529411764,\n",
       "   'f1': 0.6399999999999999,\n",
       "   'number': 17},\n",
       "  'Intervention Administration': {'precision': 0.5,\n",
       "   'recall': 1.0,\n",
       "   'f1': 0.6666666666666666,\n",
       "   'number': 1},\n",
       "  'Intervention Duration': {'precision': 0.5,\n",
       "   'recall': 0.6666666666666666,\n",
       "   'f1': 0.5714285714285715,\n",
       "   'number': 3},\n",
       "  'Intervention Frequency': {'precision': 0.3333333333333333,\n",
       "   'recall': 0.18181818181818182,\n",
       "   'f1': 0.23529411764705885,\n",
       "   'number': 11},\n",
       "  'Non-Pharmaceutical Intervention': {'precision': 0.0,\n",
       "   'recall': 0.0,\n",
       "   'f1': 0.0,\n",
       "   'number': 0},\n",
       "  'Non-Study Drug': {'precision': 0.4,\n",
       "   'recall': 1.0,\n",
       "   'f1': 0.5714285714285715,\n",
       "   'number': 2},\n",
       "  'Outcome (Study Endpoint)': {'precision': 0.7142857142857143,\n",
       "   'recall': 0.5555555555555556,\n",
       "   'f1': 0.6250000000000001,\n",
       "   'number': 18},\n",
       "  'Quantitative Measurement': {'precision': 0.8823529411764706,\n",
       "   'recall': 0.5357142857142857,\n",
       "   'f1': 0.6666666666666667,\n",
       "   'number': 28},\n",
       "  'Sample Size': {'precision': 0.7857142857142857,\n",
       "   'recall': 0.8461538461538461,\n",
       "   'f1': 0.8148148148148148,\n",
       "   'number': 13},\n",
       "  'Side Effects': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0},\n",
       "  'Statistical Significance': {'precision': 1.0,\n",
       "   'recall': 1.0,\n",
       "   'f1': 1.0,\n",
       "   'number': 8},\n",
       "  'Study Duration': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0},\n",
       "  'Type of Quant. Measure': {'precision': 1.0,\n",
       "   'recall': 0.6666666666666666,\n",
       "   'f1': 0.8,\n",
       "   'number': 6},\n",
       "  'Units': {'precision': 1.0,\n",
       "   'recall': 0.7058823529411765,\n",
       "   'f1': 0.8275862068965517,\n",
       "   'number': 17},\n",
       "  'overall_precision': 0.7480916030534351,\n",
       "  'overall_recall': 0.5833333333333334,\n",
       "  'overall_f1': 0.6555183946488294,\n",
       "  'overall_accuracy': 0.8471683475562451}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, df = compute_metrics_v2(pmids = pmids_test, \n",
    "                                 model=\"gpt-4o\"\n",
    "                                 )\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58651</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>0.978814</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.955224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>527347</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>0.824859</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.711864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>779588</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>0.920561</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.514286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1366257</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>0.832353</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.610169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1703608</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>0.729814</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.455696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pmid   model  accuracy  precision    recall        f1\n",
       "0    58651  gpt-4o  0.978814   0.969697  0.941176  0.955224\n",
       "1   527347  gpt-4o  0.824859   0.875000  0.600000  0.711864\n",
       "2   779588  gpt-4o  0.920561   0.529412  0.500000  0.514286\n",
       "3  1366257  gpt-4o  0.832353   0.666667  0.562500  0.610169\n",
       "4  1703608  gpt-4o  0.729814   0.600000  0.367347  0.455696"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instructions = \"\"\"You are a medical doctor who specializes in clinical trials and observational studies. \n",
    "    You will act as an expert annotator of research articles provided to you. You will return the strings from the article that match the tags provided. Do not convert words describing numbers into actual numbers, only return strings present in the abstract. \n",
    "    Only answer questions using data explicitly present in given studies.   \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid = 28710850\n",
    "# model = \"gpt-4o\"\n",
    "model = \"gpt-3.5-turbo-0125\"\n",
    "abstract, true_spans = retrieve_abstract_and_spans(path_to_processed_for_modeling, pmid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Efficacy and safety of beclomethasone dipropionate breath-actuated or metered-dose inhaler in pediatric patients with asthma.\\nBreath-actuated inhalers (BAI) eliminate the need for hand-breath coordination and, therefore, simplify the delivery of inhaled medication. To evaluate the efficacy and safety of beclomethasone dipropionate BAI and metered-dose inhaler (MDI) versus placebo in pediatric patients ages 4-11 years with persistent asthma. In this double-blind, double-dummy, phase III study, 628 children with persistent asthma were randomly assigned (1:1:1:1:1) to twice-daily beclomethasone dipropionate (BAI 80 μg/day, BAI 160 μg/day, MDI 80 μg/day, or MDI 160 μg/day) or to placebo. Efficacy over 12 weeks was assessed by spirometry, peak expiratory flow (PEF) measurements and other clinical end points. The primary efficacy end point was the baseline-adjusted trough morning percent predicted forced expiratory volume in 1 second (PPFEV1) area under the effect curve from 0 to 12 weeks (AUEC[0-12 weeks]). PPFEV1 AUEC(0-12 weeks) showed numerical improvements from baseline in the BAI 80 μg/day and BAI 160 μg/day groups and MDI 80 μg/day and MDI 160 μg/day groups; however, these improvements were not significant versus placebo for any group after hierarchical testing was applied. Consistent improvements were noted in the active treatment groups versus placebo for the weekly average trough morning and evening PEFs, and with BAI 80 μg/day versus placebo for rescue albuterol/salbutamol use and the total daily asthma symptom score. Most patients indicated that the BAI device was easy or very easy to use. Adverse events were comparable across the groups; the incidence of oral candidiasis ranged from 0.8 to 3.2%. Although the primary efficacy end point was not demonstrated, consistent improvements in PEF and other clinical end points were observed with beclomethasone dipropionate BAI, particularly at the 80 μg/day dose. These clinical benefits, combined with the need for better symptom control in children with asthma, supported the development of beclomethasone dipropionate BAI.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract\n",
    "# true_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spans = create_spans_with_surrounding_text(text = abstract, spans = true_spans)\n",
    "# spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.chat.completions.create(\n",
    "    model = model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_instructions},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\" \n",
    "            Task : Looking at the following abstract: {abstract65221}.\\\n",
    "            Retrieve all labels that are in : {extraction_prompts} and add the closest surrounding text (1 word on left and 1 right). \\\n",
    "            Answer : {spans65221}. \\\n",
    "            Task : Looking at the following abstract: {abstract4080}.\\\n",
    "            Retrieve all labels that are in : {extraction_prompts} and add the closest surrounding text (1 word on left and 1 right). \\\n",
    "            Answer : {spans4080}. \\\n",
    "            Task : Looking at the following abstract: {abstract29208464}.\\\n",
    "            Retrieve all labels that are in : {extraction_prompts} and add the closest surrounding text (1 word on left and 1 right). \\\n",
    "            Answer : {spans29208464}. \\\n",
    "            Task : Looking at the following abstract: {abstract30872104}.\\\n",
    "            Retrieve all labels that are in : {extraction_prompts} and add the closest surrounding text (1 word on left and 1 right). \\\n",
    "            Answer : {spans30872104}. \\\n",
    "            Task : Looking at the following abstract: {abstract35426326}.\\\n",
    "            Retrieve all labels that are in : {extraction_prompts} and add the closest surrounding text (1 word on left and 1 right). \\\n",
    "            Answer : {spans35426326}. \\\n",
    "            Task : Looking at the following abstract: {abstract}.\\\n",
    "            Retrieve all labels that are in : {extraction_prompts} and add the closest surrounding text (1 word on left and 1 right). \\\n",
    "            Answer : \n",
    "            Entities must be extracted in this format : {format_spans}.\\\n",
    "            Return the answer in a JSON with \"entities\" as key and don’t output anything beyond the required JSON file.\n",
    "            \"\"\"\n",
    "        },\n",
    "    ],\n",
    "    max_tokens= 4096,\n",
    "    temperature= 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"entities\": [\\n    {\\n      \"tag\": \"Disease/Condition of Interest\",\\n      \"text\": \"asthma\",\\n      \"surrounding\": \"patients with asthma.\"\\n    },\\n    {\\n      \"tag\": \"Drug Intervention\",\\n      \"text\": \"beclomethasone dipropionate\",\\n      \"surrounding\": \"of beclomethasone dipropionate BAI\"\\n    },\\n    {\\n      \"tag\": \"Drug Intervention\",\\n      \"text\": \"beclomethasone dipropionate\",\\n      \"surrounding\": \"beclomethasone dipropionate BAI and\"\\n    },\\n    {\\n      \"tag\": \"Sample Size\",\\n      \"text\": \"628\",\\n      \"surrounding\": \"III study, 628 children\"\\n    },\\n    {\\n      \"tag\": \"Group Characteristic\",\\n      \"text\": \"persistent asthma\",\\n      \"surrounding\": \"with persistent asthma were\"\\n    },\\n    {\\n      \"tag\": \"Intervention Frequency\",\\n      \"text\": \"twice-daily\",\\n      \"surrounding\": \"to twice-daily beclomethasone\"\\n    },\\n    {\\n      \"tag\": \"Dosage\",\\n      \"text\": \"80\",\\n      \"surrounding\": \"BAI 80 μg/day,\"\\n    },\\n    {\\n      \"tag\": \"Units\",\\n      \"text\": \"μg/day\",\\n      \"surrounding\": \"80 μg/day, BAI\"\\n    },\\n    {\\n      \"tag\": \"Dosage\",\\n      \"text\": \"160\",\\n      \"surrounding\": \"BAI 160 μg/day,\"\\n    },\\n    {\\n      \"tag\": \"Units\",\\n      \"text\": \"μg/day\",\\n      \"surrounding\": \"160 μg/day, MDI\"\\n    },\\n    {\\n      \"tag\": \"Dosage\",\\n      \"text\": \"80\",\\n      \"surrounding\": \"MDI 80 μg/day,\"\\n    },\\n    {\\n      \"tag\": \"Units\",\\n      \"text\": \"μg/day\",\\n      \"surrounding\": \"80 μg/day, or\"\\n    },\\n    {\\n      \"tag\": \"Dosage\",\\n      \"text\": \"160\",\\n      \"surrounding\": \"MDI 160 μg/day)\"\\n    },\\n    {\\n      \"tag\": \"Units\",\\n      \"text\": \"μg/day\",\\n      \"surrounding\": \"160 μg/day) or\"\\n    },\\n    {\\n      \"tag\": \"Intervention Duration\",\\n      \"text\": \"12 weeks\",\\n      \"surrounding\": \"over 12 weeks was\"\\n    },\\n    {\\n      \"tag\": \"Outcome (Study Endpoint)\",\\n      \"text\": \"spirometry\",\\n      \"surrounding\": \"was assessed by spirometry,\"\\n    },\\n    {\\n      \"tag\": \"Outcome (Study Endpoint)\",\\n      \"text\": \"peak expiratory flow\",\\n      \"surrounding\": \"peak expiratory flow (PEF)\"\\n    },\\n    {\\n      \"tag\": \"Outcome (Study Endpoint)\",\\n      \"text\": \"clinical end points\",\\n      \"surrounding\": \"and other clinical end\"\\n    },\\n    {\\n      \"tag\": \"Outcome (Study Endpoint)\",\\n      \"text\": \"baseline-adjusted trough morning percent predicted forced expiratory volume in 1 second\",\\n      \"surrounding\": \"the baseline-adjusted trough morning percent predicted forced expiratory volume in 1 second (PPFEV1)\"\\n    },\\n    {\\n      \"tag\": \"Outcome (Study Endpoint)\",\\n      \"text\": \"PPFEV1\",\\n      \"surrounding\": \"percent predicted forced expiratory volume in 1 second (PPFEV1) area\"\\n    },\\n    {\\n      \"tag\": \"Outcome (Study Endpoint)\",\\n      \"text\": \"area under the effect curve\",\\n      \"surrounding\": \"1 second (PPFEV1) area under\"\\n    },\\n    {\\n      \"tag\": \"Outcome (Study Endpoint)\",\\n      \"text\": \"AUEC\",\\n      \"surrounding\": \"under the effect curve from 0 to 12 weeks (AUEC[0-12 weeks]).\"\\n    },\\n    {\\n      \"tag\": \"Outcome (Study Endpoint)\",\\n      \"text\": \"PPFEV1 AUEC\",\\n      \"surrounding\": \"weeks (AUEC[0-12 weeks]). PPFEV1 AUEC(0-12\"\\n    },\\n    {\\n      \"tag\": \"Outcome (Study Endpoint)\",\\n      \"text\": \"weekly average trough morning and evening PEFs\",\\n      \"surrounding\": \"groups versus placebo for the weekly average trough morning and evening PEFs,\"\\n    },\\n    {\\n      \"tag\": \"Outcome (Study Endpoint)\",\\n      \"text\": \"rescue albuterol/salbutamol use\",\\n      \"surrounding\": \"versus placebo for rescue albuterol/salbutamol use and\"\\n    },\\n    {\\n      \"tag\": \"Outcome (Study Endpoint)\",\\n      \"text\": \"total daily asthma symptom score\",\\n      \"surrounding\": \"use and the total daily asthma symptom score.\"\\n    },\\n    {\\n      \"tag\": \"Side Effects\",\\n      \"text\": \"oral candidiasis\",\\n      \"surrounding\": \"the incidence of oral candidiasis ranged\"\\n    },\\n    {\\n      \"tag\": \"Quantitative Measurement\",\\n      \"text\": \"0.8\",\\n      \"surrounding\": \"candidiasis ranged from 0.8 to\"\\n    },\\n    {\\n      \"tag\": \"Quantitative Measurement\",\\n      \"text\": \"3.2%\",\\n      \"surrounding\": \"from 0.8 to 3.2%.\"\\n    },\\n    {\\n      \"tag\": \"Intervention Frequency\",\\n      \"text\": \"twice-daily\",\\n      \"surrounding\": \"to twice-daily beclomethasone\"\\n    }\\n  ]\\n}\\n```'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = completion.choices[0].message.content\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model == \"gpt-4o\" :\n",
    "    output = output.strip('```json').strip('```').strip()\n",
    "tagged_text_json = json.loads(output)\n",
    "# tagged_text_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_spans = extract_tagged_text(abstract=abstract, tagged_text=tagged_text_json[\"entities\"])\n",
    "# pred_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cye73/.conda/envs/gpt-meta-analysis/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/cye73/.conda/envs/gpt-meta-analysis/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_true_seqeval = label_tokens_from_offsets(text = abstract, annotations = true_spans)\n",
    "y_pred_seqeval = label_tokens_from_offsets(text = abstract, annotations = pred_spans)\n",
    "results = seqeval.compute(predictions=[y_pred_seqeval], references=[y_true_seqeval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision : 0.038461538461538464\n",
      "recall : 0.043478260869565216\n",
      "f1 : 0.04081632653061224\n",
      "accuracy : 0.7418546365914787\n"
     ]
    }
   ],
   "source": [
    "print(\"precision :\",results[\"overall_precision\"])\n",
    "print(\"recall :\",results[\"overall_recall\"])\n",
    "print(\"f1 :\",results[\"overall_f1\"])\n",
    "print(\"accuracy :\",results[\"overall_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option 1: Response 1 is better.\n",
      "\n",
      "Explanation: Response 1 correctly identifies and labels a wider range of relevant entities from the abstract, including the disease/condition of interest, drug intervention, sample size, group characteristic, intervention frequency, dosage, units, intervention duration, outcome (study endpoint), side effects, and quantitative measurements. It also provides the surrounding text for context. \n",
      "\n",
      "Response 2, on the other hand, mislabels some entities (e.g., \"Breath-actuated inhalers\" and \"BAI\" as the disease/condition of interest) and misses several key entities such as the sample size, group characteristic, intervention frequency, dosage, units, and quantitative measurements. Additionally, it incorrectly labels \"80 μg\" and \"day\" as the disease/condition of interest.\n",
      "\n",
      "Overall, Response 1 provides a more comprehensive and accurate extraction of the relevant information from the abstract.\n"
     ]
    }
   ],
   "source": [
    "def evaluate_extraction(model1_output, model2_output):\n",
    "    completion = openai.chat.completions.create(\n",
    "        # model=\"gpt-3.5-turbo-0125\",\n",
    "        model = \"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Your task will be to judge which of the 2 answers was better at completing the specified task.\"},\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "            Task : Looking at the following abstract: {abstract}.\\\n",
    "            Retrieve all labels that are in : {extraction_prompts} and add the closest surrounding text (1 word on left and 1 right). \\\n",
    "            Return the answer in a JSON with \"entities\" as key and don’t output anything beyond the required JSON file.\n",
    "            \n",
    "            Model 1 Output: {model1_output}\n",
    "            Model 2 Output: {model2_output}\n",
    "            \n",
    "            There are four options for you to choose from:\n",
    "            1. Response 1 is better: If you think response 1 has an advantage, then choose this option.\n",
    "            2. Response 1 is slightly better: Response 1 is very marginally better than response 2 and the difference is small. (only pick this if it's truly close)\n",
    "            3. Response 2 is slightly better: Response 2 is very marginally better than response 1 and the difference is small. (only pick this if it's truly close)\n",
    "            4. Response 2 is better: If you think response 2 has an advantage, then choose this option.\n",
    "            Explain your answer.\n",
    "            \"\"\"\n",
    "            },\n",
    "        ],\n",
    "        max_tokens=1024,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "evaluation = evaluate_extraction(model1_output=pred_spans, model2_output=true_spans)\n",
    "print(evaluation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instructions2 = \"\"\"You are a medical doctor who specializes in clinical trials and observational studies. \n",
    "    You will act as an expert annotator of research articles provided to you. Given what another professional annotator has already done, you will complete his work and correct his annotations if there are any mistakes.\n",
    "    Only answer questions using data explicitly present in given studies.   \n",
    "    \"\"\"\n",
    "# model = \"gpt-3.5-turbo-0125\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "completion2 = openai.chat.completions.create(\n",
    "    model = model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_instructions2},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\" \n",
    "            Task : Looking at the following abstract: {abstract65221}.\\\n",
    "            Retrieve all labels that are in : {extraction_prompts} and add the closest surrounding text (1 word on left and 1 right). \\\n",
    "            Answer : {spans65221}. \\\n",
    "            Task : Looking at the following abstract: {abstract29208464}.\\\n",
    "            Retrieve all labels that are in : {extraction_prompts} and add the closest surrounding text (1 word on left and 1 right). \\\n",
    "            Answer : {spans30872104}. \\\n",
    "            Task : Looking at the following abstract: {abstract}.\\\n",
    "            Retrieve all labels that are in : {extraction_prompts} and add the closest surrounding text (1 word on left and 1 right). \\\n",
    "            That's the work made by the other professional annotator : {pred_spans}.\\\n",
    "            Entities must be extracted in this format : {format_spans}.\\\n",
    "            Return the answer in a JSON with \"entities\" as key and don’t output anything beyond the required JSON file.\n",
    "            \"\"\"\n",
    "        },\n",
    "    ],\n",
    "    max_tokens= 2048,\n",
    "    temperature= 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"entities\": [\\n    {\\n      \"tag\": \"Disease/Condition of Interest\",\\n      \"text\": \"steroid-dependent asthma\",\\n      \"surrounding\": \"adult patients with steroid-dependent\"\\n    },\\n    {\\n      \"tag\": \"Sample Size\",\\n      \"text\": \"Seventy-three\",\\n      \"surrounding\": \"Seventy-three adult, steroid-dependent\"\\n    },\\n    {\\n      \"tag\": \"Study Duration\",\\n      \"text\": \"16-wk\",\\n      \"surrounding\": \"a 16-wk, double-blind\"\\n    },\\n    {\\n      \"tag\": \"Drug Intervention\",\\n      \"text\": \"flunisolide\",\\n      \"surrounding\": \"testing the efficacy of flunisolide\"\\n    },\\n    {\\n      \"tag\": \"Sample Size\",\\n      \"text\": \"Forty\",\\n      \"surrounding\": \"Forty received flunisolide,\"\\n    },\\n    {\\n      \"tag\": \"Drug Intervention\",\\n      \"text\": \"flunisolide\",\\n      \"surrounding\": \"received flunisolide, and\"\\n    },\\n    {\\n      \"tag\": \"Sample Size\",\\n      \"text\": \"33\",\\n      \"surrounding\": \"and 33 received placebo.\"\\n    },\\n    {\\n      \"tag\": \"Outcome (Study Endpoint)\",\\n      \"text\": \"mean daily prednisone requirement\",\\n      \"surrounding\": \"The mean daily prednisone requirement\"\\n    },\\n    {\\n      \"tag\": \"Quantitative Measurement\",\\n      \"text\": \"59.2%\",\\n      \"surrounding\": \"fell 59.2% during\"\\n    },\\n    {\\n      \"tag\": \"Quantitative Measurement\",\\n      \"text\": \"19.7%\",\\n      \"surrounding\": \"fell 19.7%. The\"\\n    },\\n    {\\n      \"tag\": \"Outcome (Study Endpoint)\",\\n      \"text\": \"median daily prednisone dose\",\\n      \"surrounding\": \"The median daily prednisone dose\"\\n    },\\n    {\\n      \"tag\": \"Quantitative Measurement\",\\n      \"text\": \"74.4%\",\\n      \"surrounding\": \"dropped 74.4% in\"\\n    },\\n    {\\n      \"tag\": \"Group Name\",\\n      \"text\": \"flunisolide group\",\\n      \"surrounding\": \"in the flunisolide group\"\\n    },\\n    {\\n      \"tag\": \"Quantitative Measurement\",\\n      \"text\": \"4.2%\",\\n      \"surrounding\": \"dropped 4.2% in\"\\n    },\\n    {\\n      \"tag\": \"Group Name\",\\n      \"text\": \"placebo group\",\\n      \"surrounding\": \"in the placebo group\"\\n    },\\n    {\\n      \"tag\": \"Statistical Significance\",\\n      \"text\": \"p = 0.006\",\\n      \"surrounding\": \"group (p = 0.006).\"\\n    },\\n    {\\n      \"tag\": \"Quantitative Measurement\",\\n      \"text\": \"75%\",\\n      \"surrounding\": \"group 75% tapered\"\\n    },\\n    {\\n      \"tag\": \"Quantitative Measurement\",\\n      \"text\": \"50%\",\\n      \"surrounding\": \"use of oral steroids 50%\"\\n    },\\n    {\\n      \"tag\": \"Quantitative Measurement\",\\n      \"text\": \"27.5%\",\\n      \"surrounding\": \"or more, and 27.5%\"\\n    },\\n    {\\n      \"tag\": \"Quantitative Measurement\",\\n      \"text\": \"36%\",\\n      \"surrounding\": \"group 36% tapered\"\\n    },\\n    {\\n      \"tag\": \"Quantitative Measurement\",\\n      \"text\": \"12%\",\\n      \"surrounding\": \"or more, and only 12%\"\\n    },\\n    {\\n      \"tag\": \"Outcome (Study Endpoint)\",\\n      \"text\": \"reduction in the daily severity of wheezing\",\\n      \"surrounding\": \"achieved significantly greater reduction in the daily severity of wheezing\"\\n    },\\n    {\\n      \"tag\": \"Statistical Significance\",\\n      \"text\": \"p = 0.014\",\\n      \"surrounding\": \"wheezing (p = 0.014)\"\\n    },\\n    {\\n      \"tag\": \"Outcome (Study Endpoint)\",\\n      \"text\": \"frequency of asthma attacks\",\\n      \"surrounding\": \"and frequency of asthma attacks\"\\n    },\\n    {\\n      \"tag\": \"Statistical Significance\",\\n      \"text\": \"p = 0.049\",\\n      \"surrounding\": \"attacks (p = 0.049)\"\\n    },\\n    {\\n      \"tag\": \"Quantitative Measurement\",\\n      \"text\": \"70%\",\\n      \"surrounding\": \"response, 70% of\"\\n    },\\n    {\\n      \"tag\": \"Outcome (Study Endpoint)\",\\n      \"text\": \"very good or good response\",\\n      \"surrounding\": \"as having a very good or good response\"\\n    },\\n    {\\n      \"tag\": \"Quantitative Measurement\",\\n      \"text\": \"30%\",\\n      \"surrounding\": \"response, and 30% were\"\\n    },\\n    {\\n      \"tag\": \"Outcome (Study Endpoint)\",\\n      \"text\": \"fair or poor response\",\\n      \"surrounding\": \"as having a fair or poor response\"\\n    },\\n    {\\n      \"tag\": \"Quantitative Measurement\",\\n      \"text\": \"33%\",\\n      \"surrounding\": \"In contrast 33% of\"\\n    },\\n    {\\n      \"tag\": \"Outcome (Study Endpoint)\",\\n      \"text\": \"very good or good\",\\n      \"surrounding\": \"were rated as very good or good\"\\n    },\\n    {\\n      \"tag\": \"Quantitative Measurement\",\\n      \"text\": \"67%\",\\n      \"surrounding\": \"good, and 67% were\"\\n    },\\n    {\\n      \"tag\": \"Outcome (Study Endpoint)\",\\n      \"text\": \"fair or poor\",\\n      \"surrounding\": \"were rated as fair or poor\"\\n    },\\n    {\\n      \"tag\": \"Statistical Significance\",\\n      \"text\": \"p = 0.0009\",\\n      \"surrounding\": \"poor (p = 0.0009)\"\\n    },\\n    {\\n      \"tag\": \"Outcome (Study Endpoint)\",\\n      \"text\": \"Plasma cortisols\",\\n      \"surrounding\": \"Plasma cortisols showed\"\\n    },\\n    {\\n      \"tag\": \"Quantitative Measurement\",\\n      \"text\": \"42.9%\",\\n      \"surrounding\": \"an average increase of 42.9%\"\\n    }\\n  ]\\n}\\n```'"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2 = completion2.choices[0].message.content\n",
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model == \"gpt-4o\" :\n",
    "    output2 = output2.strip('```json').strip('```').strip()\n",
    "tagged_text_json2 = json.loads(output2)\n",
    "# tagged_text_json2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 90,\n",
       "  'end': 103,\n",
       "  'label': 7,\n",
       "  'tag': 'Sample Size',\n",
       "  'text': 'Seventy-three'},\n",
       " {'start': 166,\n",
       "  'end': 171,\n",
       "  'label': 19,\n",
       "  'tag': 'Study Duration',\n",
       "  'text': '16-wk'},\n",
       " {'start': 216,\n",
       "  'end': 227,\n",
       "  'label': 1,\n",
       "  'tag': 'Drug Intervention',\n",
       "  'text': 'flunisolide'},\n",
       " {'start': 237, 'end': 242, 'label': 7, 'tag': 'Sample Size', 'text': 'Forty'},\n",
       " {'start': 252,\n",
       "  'end': 263,\n",
       "  'label': 1,\n",
       "  'tag': 'Drug Intervention',\n",
       "  'text': 'flunisolide'},\n",
       " {'start': 269, 'end': 271, 'label': 7, 'tag': 'Sample Size', 'text': '33'},\n",
       " {'start': 294,\n",
       "  'end': 327,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'mean daily prednisone requirement'},\n",
       " {'start': 367,\n",
       "  'end': 372,\n",
       "  'label': 0,\n",
       "  'tag': 'Quantitative Measurement',\n",
       "  'text': '59.2%'},\n",
       " {'start': 448,\n",
       "  'end': 453,\n",
       "  'label': 0,\n",
       "  'tag': 'Quantitative Measurement',\n",
       "  'text': '19.7%'},\n",
       " {'start': 459,\n",
       "  'end': 487,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'median daily prednisone dose'},\n",
       " {'start': 496,\n",
       "  'end': 501,\n",
       "  'label': 0,\n",
       "  'tag': 'Quantitative Measurement',\n",
       "  'text': '74.4%'},\n",
       " {'start': 509,\n",
       "  'end': 526,\n",
       "  'label': 3,\n",
       "  'tag': 'Group Name',\n",
       "  'text': 'flunisolide group'},\n",
       " {'start': 543,\n",
       "  'end': 556,\n",
       "  'label': 3,\n",
       "  'tag': 'Group Name',\n",
       "  'text': 'placebo group'},\n",
       " {'start': 558,\n",
       "  'end': 567,\n",
       "  'label': 5,\n",
       "  'tag': 'Statistical Significance',\n",
       "  'text': 'p = 0.006'},\n",
       " {'start': 595,\n",
       "  'end': 598,\n",
       "  'label': 0,\n",
       "  'tag': 'Quantitative Measurement',\n",
       "  'text': '75%'},\n",
       " {'start': 628,\n",
       "  'end': 631,\n",
       "  'label': 0,\n",
       "  'tag': 'Quantitative Measurement',\n",
       "  'text': '50%'},\n",
       " {'start': 645,\n",
       "  'end': 650,\n",
       "  'label': 0,\n",
       "  'tag': 'Quantitative Measurement',\n",
       "  'text': '27.5%'},\n",
       " {'start': 713,\n",
       "  'end': 716,\n",
       "  'label': 0,\n",
       "  'tag': 'Quantitative Measurement',\n",
       "  'text': '36%'},\n",
       " {'start': 768,\n",
       "  'end': 771,\n",
       "  'label': 0,\n",
       "  'tag': 'Quantitative Measurement',\n",
       "  'text': '12%'},\n",
       " {'start': 918,\n",
       "  'end': 961,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'reduction in the daily severity of wheezing'},\n",
       " {'start': 963,\n",
       "  'end': 972,\n",
       "  'label': 5,\n",
       "  'tag': 'Statistical Significance',\n",
       "  'text': 'p = 0.014'},\n",
       " {'start': 978,\n",
       "  'end': 1005,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'frequency of asthma attacks'},\n",
       " {'start': 1007,\n",
       "  'end': 1016,\n",
       "  'label': 5,\n",
       "  'tag': 'Statistical Significance',\n",
       "  'text': 'p = 0.049'},\n",
       " {'start': 1101,\n",
       "  'end': 1104,\n",
       "  'label': 0,\n",
       "  'tag': 'Quantitative Measurement',\n",
       "  'text': '70%'},\n",
       " {'start': 1162,\n",
       "  'end': 1188,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'very good or good response'},\n",
       " {'start': 1194,\n",
       "  'end': 1197,\n",
       "  'label': 0,\n",
       "  'tag': 'Quantitative Measurement',\n",
       "  'text': '30%'},\n",
       " {'start': 1221,\n",
       "  'end': 1242,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'fair or poor response'},\n",
       " {'start': 1256,\n",
       "  'end': 1259,\n",
       "  'label': 0,\n",
       "  'tag': 'Quantitative Measurement',\n",
       "  'text': '33%'},\n",
       " {'start': 1304,\n",
       "  'end': 1321,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'very good or good'},\n",
       " {'start': 1327,\n",
       "  'end': 1330,\n",
       "  'label': 0,\n",
       "  'tag': 'Quantitative Measurement',\n",
       "  'text': '67%'},\n",
       " {'start': 1345,\n",
       "  'end': 1357,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'fair or poor'},\n",
       " {'start': 1359,\n",
       "  'end': 1369,\n",
       "  'label': 5,\n",
       "  'tag': 'Statistical Significance',\n",
       "  'text': 'p = 0.0009'},\n",
       " {'start': 1408,\n",
       "  'end': 1424,\n",
       "  'label': 2,\n",
       "  'tag': 'Outcome (Study Endpoint)',\n",
       "  'text': 'Plasma cortisols'},\n",
       " {'start': 1455,\n",
       "  'end': 1460,\n",
       "  'label': 0,\n",
       "  'tag': 'Quantitative Measurement',\n",
       "  'text': '42.9%'}]"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_spans2 = extract_tagged_text(abstract=abstract, tagged_text=tagged_text_json2[\"entities\"])\n",
    "pred_spans2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_seqeval2 = label_tokens_from_offsets(text = abstract, annotations = true_spans)\n",
    "y_pred_seqeval2 = label_tokens_from_offsets(text = abstract, annotations = pred_spans2)\n",
    "results2 = seqeval.compute(predictions=[y_pred_seqeval2], references=[y_true_seqeval2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision : 0.6470588235294118\n",
      "recall : 0.4489795918367347\n",
      "f1 : 0.5301204819277109\n",
      "accuracy : 0.8012820512820513\n"
     ]
    }
   ],
   "source": [
    "print(\"precision :\",results2[\"overall_precision\"])\n",
    "print(\"recall :\",results2[\"overall_recall\"])\n",
    "print(\"f1 :\",results2[\"overall_f1\"])\n",
    "print(\"accuracy :\",results2[\"overall_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
